---
title: "6289_final_3"
author: "rballard myork apaladino"
date: "2025-04-15"
output: html_document
---


# Project 3: Flood Risk Mismatch in the Mid-Atlantic

## Frontmatter

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(20052)

#install.packages("rfema", repos = "https://ropensci.r-universe.dev")

pkg_vector <- c("AOI","INLA","NFHL","broom","dplyr","exactextractr","ggbreak",
                "ggplot2","htmltools","htmlwidgets","httr","leaflet",
                "leaflet.extras","lmtest","purrr","raster","readr","reader",
                "rfema","rvest","sf","spData","spdep","spatialreg","sphet",
                "stringr","terra","tigris","tidycensus","tidyr","tmap","xml2")

# Install missing packages
missing_packages <- setdiff(pkg_vector, rownames(installed.packages()))
if (length(missing_packages)) install.packages(missing_packages)

# Load all packages
invisible(lapply(pkg_vector, function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))))


#User needs to set their own data_dir if working directory not repo base
data_dir <- paste0(getwd(),'/data')
```

# 1. FEMA Flood Zone Extraction

- Extract Layer 28 polygons for the DMV region using the `NFHL` R package.
- Filter for **Special Flood Hazard Areas (SFHA)** using the condition `SFHA_TF == "T"`.

```{r createnfhldf,eval=FALSE}

# FEMA NFHL base URL
base_url <- "https://hazards.fema.gov/femaportal/NFHL/"
target_url <- paste0(base_url, "searchResult")

# Read page
page <- read_html(target_url)

# Extract all table rows
rows <- page %>% html_nodes("tr")

# Parse each row into fields: FIRMID, County, State, Date, Size, Link
nfhl_df <- rows %>%
  map_df(~{
    tds <- .x %>% html_nodes("td")
    if (length(tds) < 6) return(NULL)  # skip malformed rows

    list(
      dfirm_id = tds[1] %>% html_text(trim = TRUE),
      county = tds[2] %>% html_text(trim = TRUE),
      state = tds[3] %>% html_text(trim = TRUE),
      effective_date = tds[4] %>% html_text(trim = TRUE),
      file_size = tds[5] %>% html_text(trim = TRUE),
      url = tds[6] %>% html_node("a") %>% html_attr("href") %>% paste0(base_url, .)
    )
  }) %>%
  filter(str_detect(dfirm_id, "^(11|24|51).*")) %>%
  mutate(url = URLencode(url, reserved = TRUE))

# Preview
print(head(nfhl_df))

```
The above chunk extracts the html for Fema's NFHL viewer and parses it into a dataframe listing:

* dfirm_id - 6 digit FEMA identifier for locality (approximate mapping to FIPS5)
* county - County Name (contains carriage returns)
* state - State Name
* effective_date - Date the NFHL for the DFIRM was compiled/effective as of
* file_size - Size of NFHL zip for DFIRM
* url - Download URL for DFIRM


```{r nfhl_scrape, eval=FALSE}

# Base URL and scrape target
base_url <- "https://hazards.fema.gov/femaportal/NFHL/"
target_url <- paste0(base_url, "searchResult")

# Read and parse HTML
page <- read_html(target_url)

# Extract all download links with their file names
zip_links <- page %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  na.omit() %>%
  str_subset("^Download/ProductsDownLoadServlet\\?DFIRMID=") %>%
  unique()

# Filter by DFIRMID prefix: 11, 24, or 51
filtered_links <- zip_links %>%
  str_subset("^Download/ProductsDownLoadServlet\\?DFIRMID=(11|24|51)")

# Convert to full URLs
full_urls <- paste0(base_url, filtered_links)
full_urls <- gsub(" ", "%20", full_urls)


# Optional: Set destination folder
dest_dir <- paste0(data_dir,"/nfhl_zips")
dir.create(dest_dir, showWarnings = FALSE)

# Download function with polite wait
download_nfhl_zip <- function(url, delay = 3) {
  file_name <- str_extract(url, "fileName=[^&]+") %>%
    str_remove("fileName=")
  dest_file <- file.path(dest_dir, file_name)

  if (!file.exists(dest_file)) {
    message("Downloading: ", file_name)
    tryCatch({
      download.file(url, destfile = dest_file, mode = "wb")
      Sys.sleep(delay)
    }, error = function(e) {
      message("Error downloading ", file_name)
    })
  } else {
    message("Already downloaded: ", file_name)
  }
}

# Loop and download
walk(full_urls, download_nfhl_zip)

```

The above codeblock scrapes FEMA's NFHL DFIRM6 catalog. Only needs to be run if shapefile zips are not currently in user data directory.

```{r create_geoidpivot, eval=FALSE}

library(sf)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)

# Directory containing your zip files
zip_dir <- paste0(data_dir,'/nfhl_zips')

# List of zip files (assuming they start with 5-digit GEOID prefix)
zip_files <- list.files(zip_dir, pattern = "^\\d{5}.*\\.zip$", full.names = TRUE)

# Initialize output data frame
outputs <- tibble(GEOID = character(), FLD_ZONE = character(), COUNT = integer())

# Loop through each zip file
for (zip_path in zip_files) {
  # Extract the 5-digit GEOID from the filename
  geoid <- str_extract(basename(zip_path), "^\\d{5}")

  # Create temporary directory for extraction
  temp_dir <- tempfile(pattern = "nfhl_")
  dir.create(temp_dir)

  # Extract contents
  unzip(zip_path, exdir = temp_dir)

  # Look for the S_FLD_HAZ_AR.shp shapefile
  shp_files <- list.files(temp_dir, pattern = "S_FLD_HAZ_AR\\.shp$", recursive = TRUE, full.names = TRUE)
  if (length(shp_files) == 0) {
    message(glue::glue("No shapefile found in {zip_path}, skipping."))
    unlink(temp_dir, recursive = TRUE)
    next
  }

  # Load shapefile
  flood_layer <- tryCatch({
    st_read(shp_files[1], quiet = TRUE)
  }, error = function(e) {
    message(glue::glue("Failed to read shapefile in {zip_path}, skipping."))
    unlink(temp_dir, recursive = TRUE)
    return(NULL)
  })

  if (is.null(flood_layer)) next

  #Summarize FLD_ZONE counts
  pivot <- flood_layer %>%
    st_drop_geometry() %>%
    count(FLD_ZONE, name = "COUNT") %>%
    mutate(GEOID = geoid) %>%
    dplyr::select(GEOID, FLD_ZONE, COUNT)
  

  # Append to outputs
  outputs <- bind_rows(outputs, pivot)

  
  # Cleanup
  unlink(temp_dir, recursive = TRUE)
}

```

The above codeblock iteratively unzips the downloaded NFHL-DFIRM data into a temp directory and constructs a dataframe with GEOID/DFIRM6, a pivot of count of floodzone designations for census tract, and counts of these designations within the GEOID/DFIRM.

```{r write_geoidoutputs, eval=FALSE}
write.csv(outputs, file = paste0(data_dir,"/outputs.csv"), row.names = FALSE)
```

The above codeblock writes the resultant parsed geoid dataframe to the user data directory. Should only be run if the above codeblock transforming the shapefiles is run.


```{r load_outputs}
outputs <- read_csv(paste0(data_dir,"/outputs.csv"), col_types = cols(.default = "c"))
```

The above codeblock reads the parsed geoid dataframe to the user data directory.


# 2. NOAA Flood Event Collection

- Download and filter NOAA flood events (1950â€“2023) by event type:
  - Flood
  - Flash Flood
  - Coastal Flood

```{r noaa_bulkdatadownload, eval=FALSE}
#This code chunk will download all the NOAA event data so don't run if data is already present in environment.

# Base URL of the directory
base_url <- "https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/"

# Read the HTML content of the directory listing
page <- read_html(base_url)

# Extract all links to .csv.gz files
file_links <- page %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  na.omit() %>%
  str_subset("\\.csv\\.gz$")

# Make full URLs
full_urls <- paste0(base_url, file_links)

# Download folder (you can change this)
download_dir <- paste0(data_dir,'/nwsalerts_bulkextract')

dir.create(download_dir, showWarnings = FALSE)

# Loop to download with delays
for (url in full_urls) {
  file_name <- basename(url)
  dest_path <- file.path(download_dir, file_name)
  
  message("Downloading: ", file_name)
  tryCatch({
    download.file(url, destfile = dest_path, mode = "wb")
  }, error = function(e) {
    warning("Failed to download ", file_name, ": ", conditionMessage(e))
  })
  
  # Wait a few seconds to avoid hammering the server
  Sys.sleep(runif(1, 2, 5))  # random wait between 2 and 5 seconds
}

```

The above code chunk downloads NOAA Event Details for 1950-2024. It should only be run if data is not already present in development environment.

```{r noaa_rowjoin, eval=FALSE}
#rowwise joins of event details, fatalities, and locations


files <- list.files(data_dir, pattern = "\\.csv\\.gz$", full.names = TRUE)

details_files <- files[grepl("StormEvents_details", files)]
locations_files <- files[grepl("StormEvents_locations", files)]
fatalities_files <- files[grepl("StormEvents_fatalities", files)]

read_as_character <- function(file) {
  df <- read_csv(file, col_types = cols(.default = col_character()), show_col_types = FALSE)
  df$filename <- basename(file)
  df
}

# Combine all details files
storm_details_df <- map_dfr(details_files, read_as_character)

# Combine all locations files
storm_locations_df <- map_dfr(locations_files, read_as_character)

# Combine all fatalities files
storm_fatalities_df <- map_dfr(fatalities_files, read_as_character)

head(storm_details_df)

```

The above code chunk reads in and conducts a rowwise join of the annual NOAA event tables. Will only need to be run if a new dataset is being constructed.


```{r noaa_tablewrite, eval=FALSE}
#writes processed table

write_pipe_csv_gz <- function(df, output_dir, filename) {
  # Ensure the output directory exists
  dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
  
  # Construct full path
  file_path <- file.path(output_dir, paste0(filename, ".csv.gz"))
  
  # Write the compressed, pipe-delimited file
  readr::write_delim(
    df,
    file = file_path,
    delim = "|",
    col_names = TRUE
  )
  
  message("File written to: ", file_path)
}

write_pipe_csv_gz(storm_details_df, data_dir, "storm_details_df")
```

The above chunk will write the joined noaa event details table to data_dir. Only needs to be run if a new table of noaa events has been downloaded.


```{r noaa_tableload}
#will read the filtered storm_details_df.csv.gz saved to your repo data directory from GDrive

read_pipe_csv_gz <- function(input_dir, filename) {
  file_path <- file.path(input_dir, paste0(filename, ".csv.gz"))
  
  df <- readr::read_delim(
    file = file_path,
    delim = "|",
    col_names = TRUE,
    show_col_types = FALSE,
  col_types = cols(.default = col_character())
  )
  
  return(df)
}

storm_details_df <- as_tibble(read_pipe_csv_gz(data_dir, "storm_details_df"))
```

The above code chunk can be used to load a previously saved noaa event details .csv.gz to proceed with processing.

```{r noaa_detailsfilter}
#filters event details to include only DC, MD, VA state fips codes
storm_details_df <- storm_details_df %>%
  filter(STATE_FIPS %in% c("11", #DC
                           "24", #MD
                           "51"  #VA
                           ))

storm_details_df <- storm_details_df %>%
  filter(EVENT_TYPE %in% c('Flash Flood',
                           'Flood',
                           'Coastal Flood'
                           ))
```

The above code chunk filters the events data to include only STATE_FIPS for DC/MD/VA and to only include EVENT_TYPES of Coastal Flood, Flash Flood, or Flood.


- Extract:
  - Geolocation
  - Event date
  - Event type
  - Damage estimates

```{r noaatablesubset}

storm_details_df <- storm_details_df %>%
  dplyr::select("EVENT_ID",
                "BEGIN_YEARMONTH",
                "BEGIN_DAY",
                "BEGIN_TIME",
                "STATE",
                "STATE_FIPS",
                "YEAR",
                "MONTH_NAME",
                "EVENT_TYPE",
                "CZ_FIPS",
                "CZ_NAME",
                "BEGIN_DATE_TIME",
                "INJURIES_DIRECT",
                "INJURIES_INDIRECT",
                "DEATHS_DIRECT",
                "DEATHS_INDIRECT",
                "DAMAGE_PROPERTY",
                "DAMAGE_CROPS",
                "FLOOD_CAUSE",
                "BEGIN_LOCATION",
                "BEGIN_LAT",
                "BEGIN_LON"
)


```

The above codeblock subsets NOAA-NWS event details to include only fields germane to our analysis.


```{r noaaevents_cleaning}
storm_details_df <- storm_details_df %>%
  mutate(
    DAMAGE_CROPS_CLEAN = case_when(
      str_detect(DAMAGE_CROPS, regex("^[0-9.]+K$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_CROPS, regex("K", ignore_case = TRUE))) * 1e3,
      str_detect(DAMAGE_CROPS, regex("^[0-9.]+M$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_CROPS, regex("M", ignore_case = TRUE))) * 1e6,
      str_detect(DAMAGE_CROPS, "^[0-9.]+$") ~
        as.numeric(DAMAGE_CROPS),
      TRUE ~ NA_real_
    )
  )

storm_details_df <- storm_details_df %>%
  mutate(
    DAMAGE_PROPERTY_CLEAN = case_when(
      str_detect(DAMAGE_PROPERTY, regex("^[0-9.]+K$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_PROPERTY, regex("K", ignore_case = TRUE))) * 1e3,
      str_detect(DAMAGE_PROPERTY, regex("^[0-9.]+M$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_PROPERTY, regex("M", ignore_case = TRUE))) * 1e6,
      str_detect(DAMAGE_PROPERTY, "^[0-9.]+$") ~
        as.numeric(DAMAGE_PROPERTY),
      TRUE ~ NA_real_
    )
  )


storm_details_df <- storm_details_df %>%
  mutate(across(c(INJURIES_DIRECT, INJURIES_INDIRECT, DEATHS_DIRECT, DEATHS_INDIRECT), as.numeric))


storm_details_df <- storm_details_df %>%
  mutate(
    FIPS5 = paste0(STATE_FIPS, str_pad(CZ_FIPS, width = 3, pad = "0"))
  )

storm_details_df <- storm_details_df %>%
  mutate(
    TOTAL_DAMAGE = DAMAGE_PROPERTY_CLEAN + DAMAGE_CROPS_CLEAN
  )
```

There are certain fields within the event details data which need cleaning. For example, FIPS county (`CZ_FIPS`) is not left padded, and `DAMAGE_PROPERTY` and `DAMAGE_CROPS` have string scalars (`k`, `m`) that need to be handled.

We pad CZ_FIPS and construct the FIPS5/GEOID as a string concatenation of `STATE_FIPS` + `CZ_FIPS`

We convert the scalars to scientific notation and apply them to the damage fields. The transformed fields are named as `DAMAGE_PROPERTY_CLEAN` and `DAMAGE_CROPS_CLEAN` respectively.

Here also we tabulate `TOTAL_DAMAGE` as `DAMAGE_PROPERTY_CLEAN` + `DAMAGE_CROPS_CLEAN`.


# 3. Spatial Join and Aggregation

- Assign events to spatial units:
  - Census tracts or
  - ZIP Code Tabulation Areas (ZCTAs)
  
```{r constructlinktable}
library(tigris)
library(dplyr)

# Get all U.S. counties (this pulls GEOID, NAME, STATEFP, COUNTYFP)
county_sf <- counties(cb = TRUE, year = 2023)

# Just keep needed fields and convert to character
fips_df <- county_sf %>%
  st_drop_geometry() %>%
  transmute(
    NAME,
    STATEFP,
    COUNTYFP,
    FIPS5 = paste0(STATEFP, COUNTYFP),
    GEOID = GEOID  # This should match FIPS5 normally
  )

# Your FEMA output GEOIDs (e.g., from the earlier script)
fema_geoids <- outputs %>%
  distinct(GEOID) %>%
  pull()

# Identify GEOIDs that are in FEMA data but not in official FIPS5
fema_geoids_df <- tibble(FEMA_GEOID = fema_geoids)

lookup_table <- fema_geoids_df %>%
  anti_join(fips_df, by = c("FEMA_GEOID" = "FIPS5"))

manual_fixes <- tibble(
  FEMA_DFIRM = c("11000", #DC City
                 "24000", #Baltimore City
                 "51002", #Bristol City
                 "51004", #Cumberland County, there are two shapefile sets for this data, also exists as 51049, fips 51049
                 "51006", #Franklin City
                 "51008", #City of Hopewell
                 "51010", #City of Norfolk
                 "51012", #City of Radford
                 "51018", # City of Poquoson
                 "51551", # City of Alexandria),
                 "51540"), #City of Charlottesville
  FIPS5 = c("11001", #DC City
            "24005", #Baltimore City
            "51502", #Bristol City
            "51049", #Cumberland County
            "51620", #Franklin City
            "51670", #City of Hopewell
            "51710 ", #City of Norfolk
            "51750", #City of Radford
            "51735", #City of Poquoson
            "51510", #City of Alexandria
            "51003"
            ))

```

This link table creates logic to conduct a columnar merge between the FEMA NFHL data, which uses DFIRM6 as the primary key for municipality, and the NOAA events table, which uses FIPS5. This join is not wholly 1:1 and additional research could yield improved results or assist the analytics community as no standard appears to currently exist.


```{r manualfix_nhfl}

outputs2 <- outputs %>%
  left_join(manual_fixes, by = c("GEOID" = "FEMA_DFIRM")) %>%
  mutate(GEOID = if_else(!is.na(FIPS5), FIPS5, GEOID)) %>%
  dplyr::select(-FIPS5)

outputs2$COUNT <- as.numeric(outputs$COUNT)

outputs_wide <- outputs2 %>%
  group_by(GEOID, FLD_ZONE) %>%
  summarise(COUNT = sum(COUNT, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(
    names_from = FLD_ZONE,
    values_from = COUNT,
    values_fill = list(COUNT = 0)
  )

```

Here duplicative DFIRM values are consolidated for join. This is an artifact of independent cities within counties having distinct or recoded DFIRM6 values that do not fully align with their FIPS5.

```{r join_nfhlouttonoaaevent}

library(dplyr)

events_flood_df <- storm_details_df %>%
  left_join(outputs_wide, by = c("FIPS5" = "GEOID"))

subset_na_A <- events_flood_df %>%
  filter(is.na(A)) %>%
  dplyr::select(FIPS5, STATE, CZ_NAME)

distinct_na_A_FIPS5 <- events_flood_df %>%
  filter(is.na(A)) %>%
  dplyr::select(FIPS5, STATE, CZ_NAME) %>%
  distinct(FIPS5, .keep_all = TRUE)

```

Here we implement the join described above using the link table, and we extract unmatched IDs from each for exploration (which is how the manual lookup values were derived above)


```{r write_eventsflood, eval=FALSE}
write.csv(events_flood_df, file = paste0(data_dir,"/events_flood_df.csv"), row.names = FALSE)
```

Joined table is written. This block will not need to be run if the user has events_flood_df in their user data directory.

```{r load_eventsflood}
events_flood_df <- read_csv(paste0(data_dir,"/events_flood_df.csv"), col_types = cols(.default = "c"))

events_flood_df <- events_flood_df %>%
  mutate(
    YEAR = as.numeric(as.character(YEAR)),
    TOTAL_DAMAGE = as.numeric(as.character(TOTAL_DAMAGE)),
    DAMAGE_PROPERTY_CLEAN = as.numeric(as.character(DAMAGE_PROPERTY_CLEAN)),
    DAMAGE_CROPS_CLEAN = as.numeric(as.character(DAMAGE_CROPS_CLEAN)),
    DEATHS_DIRECT = as.numeric(as.character(DEATHS_DIRECT)),
    DEATHS_INDIRECT = as.numeric(as.character(DEATHS_INDIRECT)),
    INJURIES_DIRECT = as.numeric(as.character(INJURIES_DIRECT)),
    INJURIES_INDIRECT = as.numeric(as.character(INJURIES_INDIRECT))
  )

```

Joined table is loaded and fields retyped appropriately.


- Count flood event frequency per spatial unit.
- Overlay flood event hotspots on FEMA SFHA zones.
- Identify:
  - **Matching areas**: events occurring inside SFHA zones
  - **Mismatch areas**: events occurring outside SFHA zones

# 4. Time Series Analysis

- Aggregate flood events by decade.

```{r transform_eventsflood_decadeadd}
events_flood_df <- events_flood_df %>%
  mutate(
    DECADE = case_when(
      YEAR >= 1990 & YEAR < 2000 ~ "90s",
      YEAR >= 2000 & YEAR < 2010 ~ "00s",
      YEAR >= 2010 & YEAR < 2020 ~ "10s",
      YEAR >= 2020 & YEAR < 2030 ~ "2020-2024",
      TRUE ~ "Other"
    )
  ) %>%
  mutate(
    DECADE = factor(DECADE, levels = c("90s", "00s", "10s", "2020-2024"))
  )
```

Aggregate data by decade for 1990s to 2020s (to-date).

- Plot trends in:
  - Event frequency
  - Severity using damage amounts

```{r pivot_eventsflood_fips5decade}

decade_frequency_f <- events_flood_df %>%
  group_by(FIPS5, DECADE) %>%
  summarise(
    UNIQUE_EVENTS = n_distinct(EVENT_ID),
    .groups = "drop"
  )

# Summarize total damages by year and state
decade_damage_f <- events_flood_df %>%
  group_by(FIPS5,DECADE) %>%
  summarise(
    TOTAL_DAMAGE = sum(DAMAGE_PROPERTY_CLEAN + DAMAGE_CROPS_CLEAN, na.rm = TRUE),
    .groups = "drop"
  )

decade_casualty_f <- events_flood_df %>%
  group_by(FIPS5,DECADE) %>%
  summarise(
    TOTAL_CASUALTIES = sum(
      DEATHS_DIRECT + DEATHS_INDIRECT + INJURIES_DIRECT + INJURIES_INDIRECT,
      na.rm = TRUE),
    .groups = "drop"
  )

```

Here we create pivots for FIPS5 decade totals for damage (cost), event count, and casualties.

```{r pivot_eventsflood_totaldecade}
decade_frequency_t <- events_flood_df %>%
  group_by(DECADE) %>%
  summarise(
    UNIQUE_EVENTS = n_distinct(EVENT_ID),
    .groups = "drop"
  )

decade_damage_t <- events_flood_df %>%
  group_by(DECADE) %>%
  summarise(
    TOTAL_DAMAGE = sum(DAMAGE_PROPERTY_CLEAN + DAMAGE_CROPS_CLEAN, na.rm = TRUE),
    .groups = "drop"
  )

decade_casualty_t <- events_flood_df %>%
  group_by(DECADE) %>%
  summarise(
    TOTAL_CASUALTIES = sum(
      DEATHS_DIRECT + DEATHS_INDIRECT + INJURIES_DIRECT + INJURIES_INDIRECT,
      na.rm = TRUE),
    .groups = "drop"
  )
```

Here we create pivots for DMV decade totals for damage (cost), event count, and casualties.

```{r viz_eventsflood_decadedamage}
dmg_fips <- ggplot(decade_damage_f, aes(x = DECADE, y = log(TOTAL_DAMAGE+1))) +
  geom_boxplot(outlier.alpha = 0.3, fill = "#4682b4") +
  labs(
    title = "Distribution of Total Flood Event Damage\nby FIPS5 (County) by Decade",
    x = "Decade",
    y = "Log Total Flood Event Damage by FIPS5 (County)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

dmg_tot <- ggplot(decade_damage_t, aes(x = DECADE, y = log(TOTAL_DAMAGE + 1))) +
  geom_col(fill = "#4682b4") +
  labs(
    title = "Total Flood Damage by Decade",
    x = "Decade",
    y = "Log Total Flood Damage"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

library(gridExtra)
grid.arrange(dmg_fips, dmg_tot, ncol = 2, widths = c(1, 1))

zero_damage_count <- decade_damage_f %>%
  filter(TOTAL_DAMAGE == 0) %>%
  nrow()

fips_zerodamage <- zero_damage_count
fipscount <- length(decade_damage_f$FIPS5)

propzerodamage <- fips_zerodamage/fipscount

```

In the above FIPS5 and DMV Decade Total Flood Events time series aggregations we can see that individual counties saw a drop in estimated damage (in log dollars) due to flooding in the 2000s when compared to the 1990s, however the trend is generally positive, and 2020-2024 total damage cost is already almost equivalent to the 2010s, meaning that flooding events have become more severe over time in terms of damage expenses. A log transformation is applied as roughly `r round(propzerodamage,2)` of event observations have no estimated damage.

```{r viz_eventsflood_decadefrequency}

freq_fips <- ggplot(decade_frequency_f, aes(x = DECADE, y = UNIQUE_EVENTS)) +
  geom_boxplot(outlier.alpha = 0.3, fill = "#4682b4") +
  labs(
    title = "Distribution of Total Flood Events by\nFIPS5 (County) by Decade",
    x = "Decade",
    y = "Total Flood Events by FIPS5 (County)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

freq_tot <- ggplot(decade_frequency_t, aes(x = DECADE, y = UNIQUE_EVENTS)) +
  geom_col(fill = "#4682b4") +
  labs(
    title = "Distribution of Total Flood Events by\nFIPS5 (County) by Decade",
    x = "Decade",
    y = "Total Flood Events"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

grid.arrange(freq_fips, freq_tot, ncol = 2, widths = c(1, 1))
```
In the above FIPS5 and DMV Decade Total Flood Events time series aggregations we can see that individual counties are experiencing more frequent flooding each decade from the 90s to present, and that if the 2020s experience flooding for the remainder of the decade in line with what has already occured it will be the most flood-prone decade for the DMV of the 4 studied.

```{r viz_eventsflood_decadecasualty}

caus_fips <- ggplot(decade_casualty_f, aes(x = DECADE, y = TOTAL_CASUALTIES)) +
  geom_boxplot(outlier.alpha = 0.3, fill = "#4682b4") +
  labs(
    title = "Distribution of Total Flood Casualties by\nFIPS5 (County) by Decade",
    x = "Decade",
    y = "Total Flood Casualties by FIPS5 (County)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

caus_tot <- ggplot(decade_casualty_t, aes(x = DECADE, y = TOTAL_CASUALTIES)) +
  geom_col(fill = "#4682b4") +
  labs(
    title = "Distribution of Total Flood Casualties by Decade",
    x = "Decade",
    y = "Total Flood Casualties"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

grid.arrange(caus_fips, caus_tot, ncol = 2, widths = c(1, 1))
```

In the above FIPS5 and DMV Decade Total casualty time series aggregations we can see that individual counties generally do not experience casualties during flood events, with a handful experiencing 0-5 total throughout the decade, and that so far the 2020s appear to be experiencing lower casualty totals during flood events when compared to previous decades.

MISMATCH ANALYSIS

```{r load_floods}
library(dplyr)
library(ggplot2)
library(ggbreak)
library(sf)
library(dplyr)
library(leaflet)
library(xml2)
library(stringr)
library(rvest)
library(purrr)
library(leaflet)
library(leaflet.extras)
library(htmltools)
library(htmlwidgets)
library(tidyr)
library(shiny)

# Load in NOAA flood data
floods <- read.csv("./data/6289spatial_final/data/events_flood_df.csv")

# Aggregate floods by year
floods$year <- floods$YEAR
yearly_floods <- floods %>% group_by(year) %>% count()
yearly_floods <- data.frame(yearly_floods)

# Create dataframe of dates with 0 flood events included
data <- data.frame("year" = 1996:2024)
data <- merge(data, yearly_floods, by = "year", all.x = TRUE)
data$n[is.na(data$n)] <- 0

# Get range of x values for rectangle
x_min <- min(data$year - 1)
x_max <- max(data$year + 1)
```

Loading in the flood zones for DC, MD, and VA.
```{r load_zones}
# Load in the flood zones
flood_zones <- read.csv("./data/flood_insurance_hazard_all.csv")

# Convert WKT (Well-Known Text) POLYGON strings to sf objects
flood_zones <- st_as_sf(flood_zones, wkt = "geometry", crs = 4362)

# Set original CRS
flood_zones <- st_set_crs(flood_zones, 4269)

# If you need to transform to a specific CRS (e.g., to WGS 84)
flood_zones <- st_transform(flood_zones, 4326)
print(head(flood_zones))
```
Adding the flood points.

```{r geodc}
# Adding the flood points
flood_points_df <- data.frame("lat" = floods$BEGIN_LAT,
                              "lon" = floods$BEGIN_LON,
                              "year" = floods$YEAR,
                              "event_type" = floods$EVENT_TYPE,
                              "state" = floods$STATE)
flood_points_df <- na.omit(flood_points_df)
flood_points_sf <- st_as_sf(flood_points_df, coords = c("lon", "lat"), crs = 4326)

# Match CRS of flood zones
flood_points_sf <- st_transform(flood_points_sf, crs = st_crs(flood_zones))

# Check for validity
validity <- st_is_valid(flood_zones)

# If any are FALSE, fix them
flood_zones_fixed <- st_make_valid(flood_zones)
```

Joining flood zones and points to see if events are in the zone.

```{r checking_zone}
# Create new column to indicate whether its in the flood zone or not
floods_with_zones <- st_join(flood_points_sf, flood_zones_fixed, join = st_intersects, left = TRUE)
floods_with_zones_fixed <- st_make_valid(floods_with_zones)

# Assign flood zone status
floods_with_zones_fixed$in_flood_zone <- floods_with_zones$properties.FLD_ZONE %in% c("A", "AE", "VE")  
```

Now calculating the flood counts in each zone.
```{r floodcounts_per_zone}
# First, add polygon identifiers to each flood zone
flood_zones_fixed <- flood_zones_fixed %>%
  mutate(polygon_id = paste0("polygon_", row_number()))

# Join flood points with specific zones
# This will associate each flood point with the specific polygon it falls within
floods_with_specific_zones <- st_join(
  floods_with_zones_fixed,
  flood_zones_fixed %>% select(polygon_id, properties.FLD_ZONE),
  left = TRUE
) %>%
  mutate(
    marker_id = paste0("marker_", row_number()),
    # If point doesn't fall within any polygon, in_flood_zone will be FALSE
    in_flood_zone = (!is.na(properties.FLD_ZONE.y) & (properties.FLD_ZONE.y != "X"))
  )

# Start with your existing data preparation
floods_with_zones_fixed <- floods_with_specific_zones %>%
  mutate(
    marker_id = paste0("marker_", row_number()),
    in_flood_zone = (!is.na(properties.FLD_ZONE.y) & (properties.FLD_ZONE.y != "X"))
  )

zone_specific_stats <- floods_with_zones_fixed %>%
  st_drop_geometry() %>%
  group_by(polygon_id.y) %>%
  summarise(
    FLD_ZONE = first(properties.FLD_ZONE.y),
    n_events = n(),
    n_floods = sum(if_else(event_type == "Flood", 1, 0, missing = 0)),
    n_flash = sum(if_else(event_type == "Flash Flood", 1, 0, missing = 0)),
    n_coastal = sum(if_else(event_type == "Coastal Flood", 1, 0, missing = 0))
  )

zone_specific_stats$polygon_id <- zone_specific_stats$polygon_id.y

# Join these stats back to the original polygons
flood_zones_stats <- flood_zones_fixed %>%
  left_join(zone_specific_stats, by = "polygon_id") %>%
  mutate(
    n_events = replace_na(n_events, 0),
    zone_status = ifelse(n_events > 0, "In", "Out"),
  )
```

Now setting up specifications for DC map.
```{r dc_mis_map}
# Create zone labels
zone_labels <- c("In" = "Zone AE/A (Minimal Risk)", "Out" = "Zone X (Moderate Risk)")

# Assign zone status
flood_zones_stats$zone_status[flood_zones_stats$properties.FLD_ZONE.x=="X"] <- "Out"
flood_zones_stats$zone_status[flood_zones_stats$properties.FLD_ZONE.x!="X"] <- "In"
floods_with_zones_fixed$zone_status <- ifelse(floods_with_zones_fixed$properties.FLD_ZONE.x=="X","Out","In")

# Create color palette based on whether zone is a flood zone (Zone X) or not
pal_flood <- colorFactor(
  palette = c("Out" = "#d6c6a8", "In" = "#397fa3"),
  domain = c("Out", "In")
)

pal_zone <- colorFactor(
  palette = c("Zone AE (Moderate Risk)" = "#397fa3", "Zone X (Minimal Risk)" = "#d6c6a8"),
  domain = c("Zone AE (Moderate Risk)","Zone X (Minimal Risk)")
)

# Transform both to WGS84
flood_zones_stats <- st_transform(flood_zones_stats, crs = 4326)
floods_with_zones_fixed <- st_transform(floods_with_zones_fixed, crs = 4326)
```

Creating interactive map for DC.
```{r geodc}
# Create a lookup object for zone statistics to use in JavaScript
zone_stats_lookup <- flood_zones_stats %>%
  st_drop_geometry() %>%
  select(polygon_id, properties.FLD_ZONE, n_events) %>%
  jsonlite::toJSON()

# Add custom JavaScript for interaction
js_code <- sprintf("
function(el, x) {
  var map = this;
  
  // Parse the zone statistics lookup
  var zoneStats = %s;
  
  // Create a div for hover stats if it doesn't exist
  var statsDiv = document.getElementById('hover-stats');
  if (!statsDiv) {
    statsDiv = document.createElement('div');
    statsDiv.id = 'hover-stats';
    statsDiv.style.padding = '6px 8px';
    statsDiv.style.background = 'white';
    statsDiv.style.boxShadow = '0 0 15px rgba(0,0,0,0.2)';
    statsDiv.style.borderRadius = '5px';
    statsDiv.innerHTML = 'Hover over a zone to see statistics';
    
    var statsControl = L.control({position: 'bottomleft'});
    statsControl.onAdd = function() {
      return statsDiv;
    };
    statsControl.addTo(map);
  }
  
  // Function to update stats display
  function updateStats(polygonId) {
    for (var i = 0; i < zoneStats.length; i++) {
      if (zoneStats[i].polygon_id === polygonId) {
        var zone = zoneStats[i];
        var zoneLabel = '';
        if (zone.FLD_ZONE === 'X') zoneLabel = 'Zone X (Minimal Risk)';
        else if (zone.FLD_ZONE === 'AE') zoneLabel = 'Zone AE (High Risk)';
        else if (zone.FLD_ZONE === 'A') zoneLabel = 'Zone A (High Risk)';
        else zoneLabel = 'Zone ' + zone.FLD_ZONE;
        
        statsDiv.innerHTML = '<strong>' + zoneLabel + '</strong><br/>' +
                            'Events in this zone: ' + zone.n_events + '<br/>' +
                            'Mean depth: ' + zone.mean_depth.toFixed(2) + ' ft';
        return;
      }
    }
    // If no match found
    statsDiv.innerHTML = 'Hover over a zone to see statistics';
  }
  
  // Add event handlers to each polygon
  map.eachLayer(function(layer) {
    if (layer.options && layer.options.layerId && layer.options.layerId.startsWith('polygon_')) {
      layer.on('mouseover', function(e) {
        updateStats(layer.options.layerId);
        layer.setStyle({
          weight: 3,
          color: 'black',
          fillOpacity: 0.7
        });
        
        // Highlight all markers within this polygon
        map.eachLayer(function(markerLayer) {
          if (markerLayer.options && 
              markerLayer.options.layerId && 
              markerLayer.options.layerId.startsWith('marker_') &&
              markerLayer.feature && 
              markerLayer.feature.properties && 
              markerLayer.feature.properties.polygon_id === layer.options.layerId) {
            markerLayer.setStyle({
              radius: 7,
              fillOpacity: 1.0,
              weight: 2
            });
          }
        });
      });
      
      layer.on('mouseout', function(e) {
        statsDiv.innerHTML = 'Hover over a zone to see statistics';
        layer.setStyle({
          weight: 1,
          color: 'white',
          fillOpacity: 0.6
        });
        
        // Reset all markers
        map.eachLayer(function(markerLayer) {
          if (markerLayer.options && 
              markerLayer.options.layerId && 
              markerLayer.options.layerId.startsWith('marker_')) {
            markerLayer.setStyle({
              radius: 5,
              fillOpacity: 0.6,
              weight: 1
            });
          }
        });
      });
    }
  });
}
", zone_stats_lookup)


# Find min and max years for the palette
min_year <- min(floods_with_zones_fixed$year, na.rm = TRUE)
max_year <- max(floods_with_zones_fixed$year, na.rm = TRUE)

# 1. Year-based gradient (as discussed previously)
floods_with_zones_fixed$Year <- as.numeric(floods_with_zones_fixed$year)
min_year <- min(floods_with_zones_fixed$Year, na.rm = TRUE)
max_year <- max(floods_with_zones_fixed$Year, na.rm = TRUE)
pal_years <- colorNumeric(
  palette = colorRampPalette(c("blue", "red"))(n = 100),
  domain = c(min_year, max_year)
)

# 2. Zone status coloring
pal_zone_status <- colorFactor(
  palette = c("red", "black"),
  domain = c(TRUE, FALSE)
)

# Create the base map without any markers initially
# Check the geometry types
print(unique(st_geometry_type(flood_zones_stats)))

# Convert GEOMETRYCOLLECTION to POLYGON or MULTIPOLYGON
flood_zones_stats_fixed <- flood_zones_stats %>%
  st_cast("MULTIPOLYGON")

# Now try your map with the fixed data
m <- leaflet() %>%
  addTiles() %>%
  addControl(
    html = "<div id='hover-stats' style='padding: 6px 8px; background: white; background: rgba(255,255,255,0.8); box-shadow: 0 0 15px rgba(0,0,0,0.2); border-radius: 5px;'>Hover over a zone to see statistics</div>",
    position = "bottomleft"
  ) %>%
  addPolygons(
    data = flood_zones_stats_fixed,  # Use the fixed data here
    fillColor = ~pal_flood(zone_status),
    fillOpacity = 0.6,
    color = "white",
    weight = 1,
    layerId = ~polygon_id,
    popup = ~paste0(
      "<strong>Zone:</strong> ", zone_labels[zone_status], "<br/>",
      "<strong>Events in this specific zone:</strong> ", n_events, "<br/>",
      "<strong>Number of each event type:</strong> ", "<br/>",
      "   Floods - </strong> ", n_floods, "<br/>",
      "   Flash floods - ", n_flash, "<br/>",
      "   Coastal floods - ", n_coastal, "<br/>"
    ),
    highlightOptions = highlightOptions(
      color = "black",
      weight = 2,
      bringToFront = TRUE
    )
  )

# Create markers for Year-based coloring
year_markers <- floods_with_zones_fixed %>%
  sf::st_transform(4326) %>%
  leaflet::leaflet() %>%
  leaflet::addCircleMarkers(
    color = ~pal_years(Year),
    radius = 5,
    stroke = TRUE,
    weight = 1,
    opacity = 1,
    fillOpacity = 0.8,
    layerId = ~paste0("year_", marker_id),
    popup = ~paste0(
      "<strong>Year:</strong> ", Year, "<br/>",
      "<strong>In zone?</strong> ", ifelse(in_flood_zone, "Yes", "No"), "<br/>",
      "<strong>Zone polygon:</strong> ", ifelse(is.na(polygon_id.y), "None", polygon_id.y)
    ),
    group = "Color by Year"
  )

# Create markers for Zone-based coloring
zone_markers <- floods_with_zones_fixed %>%
  sf::st_transform(4326) %>%
  leaflet::leaflet() %>%
  leaflet::addCircleMarkers(
    color = ~ifelse(in_flood_zone, "red", "black"),
    radius = 5,
    stroke = TRUE,
    weight = 1,
    opacity = 1,
    fillOpacity = 0.8,
    layerId = ~paste0("zone_", marker_id),
    popup = ~paste0(
      "<strong>Year:</strong> ", Year, "<br/>",
      "<strong>In zone?</strong> ", ifelse(in_flood_zone, "Yes", "No"), "<br/>",
      "<strong>Zone polygon:</strong> ", ifelse(is.na(polygon_id.y), "None", polygon_id.y)
    ),
    group = "Color by Flood Zone"
  )

# Extract the marker data from both leaflet objects
year_marker_data <- year_markers$x$calls[[which(sapply(year_markers$x$calls, function(x) x$method == "addCircleMarkers"))]]
zone_marker_data <- zone_markers$x$calls[[which(sapply(zone_markers$x$calls, function(x) x$method == "addCircleMarkers"))]]

# Add both types of markers to the main map
m$x$calls <- c(m$x$calls, list(year_marker_data, zone_marker_data))

m <- m %>%
  # Add legend for flood zones
  addLegend(
    position = "bottomright",
    pal = pal_flood,
    values = flood_zones_stats$zone_status,
    title = "Flood Zone Type",
    opacity = 0.7,
    labFormat = labelFormat(prefix = "", transform = function(x) zone_labels[x])
  ) %>%
  # Add legend for years (initially hidden)
  addLegend(
    position = "bottomright",
    pal = pal_years,
    values = floods_with_zones_fixed$Year,
    title = "Flood Event Year",
    opacity = 0.7,
    # Simpler format that won't try to round non-numeric values
    labFormat = function(type, cuts, p) {
      paste0(format(cuts, big.mark = "", scientific = FALSE))
    },
    group = "Color by Year"
  ) %>%
  # Add legend for zone status (initially visible)
  addLegend(
    position = "bottomright",
    colors = c("red", "black"),
    labels = c("In Flood Zone", "Outside Flood Zone"),
    title = "Flood Event Location",
    opacity = 0.7,
    group = "Color by Flood Zone"
  ) %>%
  # Add layer control
  addLayersControl(
    baseGroups = c("Color by Flood Zone", "Color by Year"),
    options = layersControlOptions(collapsed = FALSE)
  )

# Add JavaScript to handle layer switching and legend visibility
js_code_layers <- "
function(el, x) {
  var map = this;
  
  // Set initial state - show 'Color by Flood Zone' and hide 'Color by Year'
  map.eachLayer(function(layer) {
    if (layer.options && layer.options.layerId) {
      if (layer.options.layerId.startsWith('year_')) {
        map.removeLayer(layer);
      }
    }
  });
  
  // Function to manage legend visibility
  function updateLegendVisibility() {
    var activeLayers = [];
    
    // Find which base layer is active
    document.querySelectorAll('.leaflet-control-layers-base input').forEach(function(input) {
      if (input.checked) {
        activeLayers.push(input.nextSibling.textContent.trim());
      }
    });
    
    // Show/hide legends based on active layer
    document.querySelectorAll('.leaflet-control-layers-overlays').forEach(function(legend) {
      if (legend.textContent.includes('Flood Event Year')) {
        legend.style.display = activeLayers.includes('Color by Year') ? 'block' : 'none';
      }
      if (legend.textContent.includes('Flood Event Location')) {
        legend.style.display = activeLayers.includes('Color by Flood Zone') ? 'block' : 'none';
      }
    });
  }
  
  // Add event listener for layer control
  var layerControl = document.querySelector('.leaflet-control-layers');
  if (layerControl) {
    layerControl.addEventListener('click', function() {
      setTimeout(updateLegendVisibility, 100);
    });
  }
  
  // Initial legend visibility
  setTimeout(updateLegendVisibility, 100);
}
"

# Append this JavaScript to the existing JS code
js_code_combined <- paste(js_code, js_code_layers, sep = "\n\n")

# Render the map with the combined JavaScript
map_with_js <- htmlwidgets::onRender(m, js_code_combined)
map_with_js
```

Now creating the timeseries of events across the DMV.

```{r timeseries_plot}
ui <- fluidPage(
  titlePanel("Flood Events by Year"),
  
  sidebarLayout(
    sidebarPanel(
      sliderInput("selected_year", "Select Year:",
                  min = min(floods_with_zones_fixed$year, na.rm = TRUE),
                  max = max(floods_with_zones_fixed$year, na.rm = TRUE),
                  value = min(floods_with_zones_fixed$year, na.rm = TRUE),
                  sep = "", step = 1,
                  animate = animationOptions(interval = 1500, loop = TRUE)),
      
      # Summary box
      uiOutput("year_summary")
    ),
    
    mainPanel(
      leafletOutput("flood_map", height = 600)
    )
  )
)

server <- function(input, output, session) {
  
  filtered_data <- reactive({
    floods_with_zones_fixed %>%
      filter(year == input$selected_year)
  })
  
  output$flood_map <- renderLeaflet({
    leaflet() %>%
      addProviderTiles(providers$CartoDB.Positron)
  })
  
  observe({
    leafletProxy("flood_map", data = filtered_data()) %>%
      clearMarkers() %>%
      addCircleMarkers(
        radius = 5,
        stroke = TRUE,
        weight = 1,
        color = ~ifelse(in_flood_zone, "red", "black"),
        fillOpacity = 0.8,
        popup = ~paste0(
          "<strong>Year:</strong> ", year, "<br/>",
          "<strong>Event Type:</strong> ", event_type, "<br/>",
          "<strong>In FEMA Flood Zone?</strong> ", ifelse(in_flood_zone, "Yes", "No")
        )
      )
  })
  
  output$year_summary <- renderUI({
    data <- filtered_data()
    total_events <- nrow(data)
    events_in_zone <- sum(data$in_flood_zone, na.rm = TRUE)
    events_outside_zone <- sum(!data$in_flood_zone, na.rm = TRUE)
    percent_in_zone <- ifelse(total_events > 0, round(100 * events_in_zone / total_events, 1), 0)
    
    HTML(paste0(
      "<div style='margin-top: 15px; padding: 10px; background-color: #f9f9f9; border: 1px solid #ccc; border-radius: 5px;'>",
      "<strong>Summary for ", input$selected_year, ":</strong><br/>",
      "Total Events: ", total_events, "<br/>",
      "In Flood Zone: ", events_in_zone, " (", percent_in_zone, "%)<br/>",
      "Outside Flood Zone: ", events_outside_zone,
      "</div>"
    ))
  })
}

shinyApp(ui, server)
```
