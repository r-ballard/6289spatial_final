---
title: "6289_final_3"
author: "rballard myork apaladino"
date: "2025-04-15"
output: html_document
---


# Project 3: Flood Risk Mismatch in the Mid-Atlantic

## Frontmatter

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(20052)

#install.packages("rfema", repos = "https://ropensci.r-universe.dev")

pkg_vector <- c("AOI","INLA","NFHL","broom","dplyr","exactextractr","ggbreak",
                "ggplot2","htmltools","htmlwidgets","httr","leaflet",
                "leaflet.extras","lmtest","purrr","raster","readr","reader",
                "rfema","rvest","sf","spData","spdep","spatialreg","sphet",
                "stringr","terra","tigris","tidycensus","tidyr","tmap","xml2")

# Install missing packages
missing_packages <- setdiff(pkg_vector, rownames(installed.packages()))
if (length(missing_packages)) install.packages(missing_packages)

# Load all packages
invisible(lapply(pkg_vector, function(pkg) suppressPackageStartupMessages(library(pkg, character.only = TRUE))))


#User needs to set their own data_dir if working directory not repo base
data_dir <- paste0(getwd(),'/data')
```

# 1. FEMA Flood Zone Extraction

- Extract Layer 28 polygons for the DMV region using the `NFHL` R package.
- Filter for **Special Flood Hazard Areas (SFHA)** using the condition `SFHA_TF == "T"`.

```{r createnfhldf,eval=FALSE}

# FEMA NFHL base URL
base_url <- "https://hazards.fema.gov/femaportal/NFHL/"
target_url <- paste0(base_url, "searchResult")

# Read page
page <- read_html(target_url)

# Extract all table rows
rows <- page %>% html_nodes("tr")

# Parse each row into fields: FIRMID, County, State, Date, Size, Link
nfhl_df <- rows %>%
  map_df(~{
    tds <- .x %>% html_nodes("td")
    if (length(tds) < 6) return(NULL)  # skip malformed rows

    list(
      dfirm_id = tds[1] %>% html_text(trim = TRUE),
      county = tds[2] %>% html_text(trim = TRUE),
      state = tds[3] %>% html_text(trim = TRUE),
      effective_date = tds[4] %>% html_text(trim = TRUE),
      file_size = tds[5] %>% html_text(trim = TRUE),
      url = tds[6] %>% html_node("a") %>% html_attr("href") %>% paste0(base_url, .)
    )
  }) %>%
  filter(str_detect(dfirm_id, "^(11|24|51).*")) %>%
  mutate(url = URLencode(url, reserved = TRUE))

# Preview
print(head(nfhl_df))

```
The above chunk extracts the html for Fema's NFHL viewer and parses it into a dataframe listing:

* dfirm_id - 6 digit FEMA identifier for locality (approximate mapping to FIPS5)
* county - County Name (contains carriage returns)
* state - State Name
* effective_date - Date the NFHL for the DFIRM was compiled/effective as of
* file_size - Size of NFHL zip for DFIRM
* url - Download URL for DFIRM


```{r nfhl_scrape, eval=FALSE}

# Base URL and scrape target
base_url <- "https://hazards.fema.gov/femaportal/NFHL/"
target_url <- paste0(base_url, "searchResult")

# Read and parse HTML
page <- read_html(target_url)

# Extract all download links with their file names
zip_links <- page %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  na.omit() %>%
  str_subset("^Download/ProductsDownLoadServlet\\?DFIRMID=") %>%
  unique()

# Filter by DFIRMID prefix: 11, 24, or 51
filtered_links <- zip_links %>%
  str_subset("^Download/ProductsDownLoadServlet\\?DFIRMID=(11|24|51)")

# Convert to full URLs
full_urls <- paste0(base_url, filtered_links)
full_urls <- gsub(" ", "%20", full_urls)


# Optional: Set destination folder
dest_dir <- paste0(data_dir,"/nfhl_zips")
dir.create(dest_dir, showWarnings = FALSE)

# Download function with polite wait
download_nfhl_zip <- function(url, delay = 3) {
  file_name <- str_extract(url, "fileName=[^&]+") %>%
    str_remove("fileName=")
  dest_file <- file.path(dest_dir, file_name)

  if (!file.exists(dest_file)) {
    message("Downloading: ", file_name)
    tryCatch({
      download.file(url, destfile = dest_file, mode = "wb")
      Sys.sleep(delay)
    }, error = function(e) {
      message("Error downloading ", file_name)
    })
  } else {
    message("Already downloaded: ", file_name)
  }
}

# Loop and download
walk(full_urls, download_nfhl_zip)

```

The above codeblock scrapes FEMA's NFHL DFIRM6 catalog. Only needs to be run if shapefile zips are not currently in user data directory.

```{r create_geoidpivot, eval=FALSE}

library(sf)
library(dplyr)
library(stringr)
library(tidyr)
library(purrr)

# Directory containing your zip files
zip_dir <- paste0(data_dir,'/nfhl_zips')

# List of zip files (assuming they start with 5-digit GEOID prefix)
zip_files <- list.files(zip_dir, pattern = "^\\d{5}.*\\.zip$", full.names = TRUE)

# Initialize output data frame
outputs <- tibble(GEOID = character(), FLD_ZONE = character(), COUNT = integer())

# Loop through each zip file
for (zip_path in zip_files) {
  # Extract the 5-digit GEOID from the filename
  geoid <- str_extract(basename(zip_path), "^\\d{5}")

  # Create temporary directory for extraction
  temp_dir <- tempfile(pattern = "nfhl_")
  dir.create(temp_dir)

  # Extract contents
  unzip(zip_path, exdir = temp_dir)

  # Look for the S_FLD_HAZ_AR.shp shapefile
  shp_files <- list.files(temp_dir, pattern = "S_FLD_HAZ_AR\\.shp$", recursive = TRUE, full.names = TRUE)
  if (length(shp_files) == 0) {
    message(glue::glue("No shapefile found in {zip_path}, skipping."))
    unlink(temp_dir, recursive = TRUE)
    next
  }

  # Load shapefile
  flood_layer <- tryCatch({
    st_read(shp_files[1], quiet = TRUE)
  }, error = function(e) {
    message(glue::glue("Failed to read shapefile in {zip_path}, skipping."))
    unlink(temp_dir, recursive = TRUE)
    return(NULL)
  })

  if (is.null(flood_layer)) next

  #Summarize FLD_ZONE counts
  pivot <- flood_layer %>%
    st_drop_geometry() %>%
    count(FLD_ZONE, name = "COUNT") %>%
    mutate(GEOID = geoid) %>%
    dplyr::select(GEOID, FLD_ZONE, COUNT)
  

  # Append to outputs
  outputs <- bind_rows(outputs, pivot)

  
  # Cleanup
  unlink(temp_dir, recursive = TRUE)
}

```

The above codeblock iteratively unzips the downloaded NFHL-DFIRM data into a temp directory and constructs a dataframe with GEOID/DFIRM6, a pivot of count of floodzone designations for census tract, and counts of these designations within the GEOID/DFIRM.

```{r write_geoidoutputs, eval=FALSE}
write.csv(outputs, file = paste0(data_dir,"/outputs.csv"), row.names = FALSE)
```

The above codeblock writes the resultant parsed geoid dataframe to the user data directory. Should only be run if the above codeblock transforming the shapefiles is run.


```{r load_outputs}
outputs <- read_csv(paste0(data_dir,"/outputs.csv"), col_types = cols(.default = "c"))
```

The above codeblock reads the parsed geoid dataframe to the user data directory.


# 2. NOAA Flood Event Collection

- Download and filter NOAA flood events (1950â€“2023) by event type:
  - Flood
  - Flash Flood
  - Coastal Flood

```{r noaa_bulkdatadownload, eval=FALSE}
#This code chunk will download all the NOAA event data so don't run if data is already present in environment.

# Base URL of the directory
base_url <- "https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/"

# Read the HTML content of the directory listing
page <- read_html(base_url)

# Extract all links to .csv.gz files
file_links <- page %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  na.omit() %>%
  str_subset("\\.csv\\.gz$")

# Make full URLs
full_urls <- paste0(base_url, file_links)

# Download folder (you can change this)
download_dir <- paste0(data_dir,'/nwsalerts_bulkextract')

dir.create(download_dir, showWarnings = FALSE)

# Loop to download with delays
for (url in full_urls) {
  file_name <- basename(url)
  dest_path <- file.path(download_dir, file_name)
  
  message("Downloading: ", file_name)
  tryCatch({
    download.file(url, destfile = dest_path, mode = "wb")
  }, error = function(e) {
    warning("Failed to download ", file_name, ": ", conditionMessage(e))
  })
  
  # Wait a few seconds to avoid hammering the server
  Sys.sleep(runif(1, 2, 5))  # random wait between 2 and 5 seconds
}

```

The above code chunk downloads NOAA Event Details for 1950-2024. It should only be run if data is not already present in development environment.

```{r noaa_rowjoin, eval=FALSE}
#rowwise joins of event details, fatalities, and locations


files <- list.files(data_dir, pattern = "\\.csv\\.gz$", full.names = TRUE)

details_files <- files[grepl("StormEvents_details", files)]
locations_files <- files[grepl("StormEvents_locations", files)]
fatalities_files <- files[grepl("StormEvents_fatalities", files)]

read_as_character <- function(file) {
  df <- read_csv(file, col_types = cols(.default = col_character()), show_col_types = FALSE)
  df$filename <- basename(file)
  df
}

# Combine all details files
storm_details_df <- map_dfr(details_files, read_as_character)

# Combine all locations files
storm_locations_df <- map_dfr(locations_files, read_as_character)

# Combine all fatalities files
storm_fatalities_df <- map_dfr(fatalities_files, read_as_character)

head(storm_details_df)

```

The above code chunk reads in and conducts a rowwise join of the annual NOAA event tables. Will only need to be run if a new dataset is being constructed.


```{r noaa_tablewrite, eval=FALSE}
#writes processed table

write_pipe_csv_gz <- function(df, output_dir, filename) {
  # Ensure the output directory exists
  dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
  
  # Construct full path
  file_path <- file.path(output_dir, paste0(filename, ".csv.gz"))
  
  # Write the compressed, pipe-delimited file
  readr::write_delim(
    df,
    file = file_path,
    delim = "|",
    col_names = TRUE
  )
  
  message("File written to: ", file_path)
}

write_pipe_csv_gz(storm_details_df, data_dir, "storm_details_df")
```

The above chunk will write the joined noaa event details table to data_dir. Only needs to be run if a new table of noaa events has been downloaded.


```{r noaa_tableload}
#will read the filtered storm_details_df.csv.gz saved to your repo data directory from GDrive

read_pipe_csv_gz <- function(input_dir, filename) {
  file_path <- file.path(input_dir, paste0(filename, ".csv.gz"))
  
  df <- readr::read_delim(
    file = file_path,
    delim = "|",
    col_names = TRUE,
    show_col_types = FALSE,
  col_types = cols(.default = col_character())
  )
  
  return(df)
}

storm_details_df <- as_tibble(read_pipe_csv_gz(data_dir, "storm_details_df"))
```

The above code chunk can be used to load a previously saved noaa event details .csv.gz to proceed with processing.

```{r noaa_detailsfilter}
#filters event details to include only DC, MD, VA state fips codes
storm_details_df <- storm_details_df %>%
  filter(STATE_FIPS %in% c("11", #DC
                           "24", #MD
                           "51"  #VA
                           ))

storm_details_df <- storm_details_df %>%
  filter(EVENT_TYPE %in% c('Flash Flood',
                           'Flood',
                           'Coastal Flood'
                           ))
```

The above code chunk filters the events data to include only STATE_FIPS for DC/MD/VA and to only include EVENT_TYPES of Coastal Flood, Flash Flood, or Flood.


- Extract:
  - Geolocation
  - Event date
  - Event type
  - Damage estimates

```{r noaatablesubset}

storm_details_df <- storm_details_df %>%
  dplyr::select("EVENT_ID",
                "BEGIN_YEARMONTH",
                "BEGIN_DAY",
                "BEGIN_TIME",
                "STATE",
                "STATE_FIPS",
                "YEAR",
                "MONTH_NAME",
                "EVENT_TYPE",
                "CZ_FIPS",
                "CZ_NAME",
                "BEGIN_DATE_TIME",
                "INJURIES_DIRECT",
                "INJURIES_INDIRECT",
                "DEATHS_DIRECT",
                "DEATHS_INDIRECT",
                "DAMAGE_PROPERTY",
                "DAMAGE_CROPS",
                "FLOOD_CAUSE",
                "BEGIN_LOCATION",
                "BEGIN_LAT",
                "BEGIN_LON"
)


```

The above codeblock subsets NOAA-NWS event details to include only fields germane to our analysis.


```{r noaaevents_cleaning}
storm_details_df <- storm_details_df %>%
  mutate(
    DAMAGE_CROPS_CLEAN = case_when(
      str_detect(DAMAGE_CROPS, regex("^[0-9.]+K$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_CROPS, regex("K", ignore_case = TRUE))) * 1e3,
      str_detect(DAMAGE_CROPS, regex("^[0-9.]+M$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_CROPS, regex("M", ignore_case = TRUE))) * 1e6,
      str_detect(DAMAGE_CROPS, "^[0-9.]+$") ~
        as.numeric(DAMAGE_CROPS),
      TRUE ~ NA_real_
    )
  )

storm_details_df <- storm_details_df %>%
  mutate(
    DAMAGE_PROPERTY_CLEAN = case_when(
      str_detect(DAMAGE_PROPERTY, regex("^[0-9.]+K$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_PROPERTY, regex("K", ignore_case = TRUE))) * 1e3,
      str_detect(DAMAGE_PROPERTY, regex("^[0-9.]+M$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_PROPERTY, regex("M", ignore_case = TRUE))) * 1e6,
      str_detect(DAMAGE_PROPERTY, "^[0-9.]+$") ~
        as.numeric(DAMAGE_PROPERTY),
      TRUE ~ NA_real_
    )
  )


storm_details_df <- storm_details_df %>%
  mutate(across(c(INJURIES_DIRECT, INJURIES_INDIRECT, DEATHS_DIRECT, DEATHS_INDIRECT), as.numeric))


storm_details_df <- storm_details_df %>%
  mutate(
    FIPS5 = paste0(STATE_FIPS, str_pad(CZ_FIPS, width = 3, pad = "0"))
  )

storm_details_df <- storm_details_df %>%
  mutate(
    TOTAL_DAMAGE = DAMAGE_PROPERTY_CLEAN + DAMAGE_CROPS_CLEAN
  )
```

There are certain fields within the event details data which need cleaning. For example, FIPS county (`CZ_FIPS`) is not left padded, and `DAMAGE_PROPERTY` and `DAMAGE_CROPS` have string scalars (`k`, `m`) that need to be handled.

We pad CZ_FIPS and construct the FIPS5/GEOID as a string concatenation of `STATE_FIPS` + `CZ_FIPS`

We convert the scalars to scientific notation and apply them to the damage fields. The transformed fields are named as `DAMAGE_PROPERTY_CLEAN` and `DAMAGE_CROPS_CLEAN` respectively.

Here also we tabulate `TOTAL_DAMAGE` as `DAMAGE_PROPERTY_CLEAN` + `DAMAGE_CROPS_CLEAN`.


# 3. Spatial Join and Aggregation

- Assign events to spatial units:
  - Census tracts or
  - ZIP Code Tabulation Areas (ZCTAs)
  
```{r constructlinktable}
library(tigris)
library(dplyr)

# Get all U.S. counties (this pulls GEOID, NAME, STATEFP, COUNTYFP)
county_sf <- counties(cb = TRUE, year = 2023)

# Just keep needed fields and convert to character
fips_df <- county_sf %>%
  st_drop_geometry() %>%
  transmute(
    NAME,
    STATEFP,
    COUNTYFP,
    FIPS5 = paste0(STATEFP, COUNTYFP),
    GEOID = GEOID  # This should match FIPS5 normally
  )

# Your FEMA output GEOIDs (e.g., from the earlier script)
fema_geoids <- outputs %>%
  distinct(GEOID) %>%
  pull()

# Identify GEOIDs that are in FEMA data but not in official FIPS5
fema_geoids_df <- tibble(FEMA_GEOID = fema_geoids)

lookup_table <- fema_geoids_df %>%
  anti_join(fips_df, by = c("FEMA_GEOID" = "FIPS5"))

manual_fixes <- tibble(
  FEMA_DFIRM = c("11000", #DC City
                 "24000", #Baltimore City
                 "51002", #Bristol City
                 "51004", #Cumberland County, there are two shapefile sets for this data, also exists as 51049, fips 51049
                 "51006", #Franklin City
                 "51008", #City of Hopewell
                 "51010", #City of Norfolk
                 "51012", #City of Radford
                 "51018", # City of Poquoson
                 "51551", # City of Alexandria),
                 "51540"), #City of Charlottesville
  FIPS5 = c("11001", #DC City
            "24005", #Baltimore City
            "51502", #Bristol City
            "51049", #Cumberland County
            "51620", #Franklin City
            "51670", #City of Hopewell
            "51710 ", #City of Norfolk
            "51750", #City of Radford
            "51735", #City of Poquoson
            "51510", #City of Alexandria
            "51003"
            ))

```

This link table creates logic to conduct a columnar merge between the FEMA NFHL data, which uses DFIRM6 as the primary key for municipality, and the NOAA events table, which uses FIPS5. This join is not wholly 1:1 and additional research could yield improved results or assist the analytics community as no standard appears to currently exist.


```{r manualfix_nhfl}

outputs2 <- outputs %>%
  left_join(manual_fixes, by = c("GEOID" = "FEMA_DFIRM")) %>%
  mutate(GEOID = if_else(!is.na(FIPS5), FIPS5, GEOID)) %>%
  dplyr::select(-FIPS5)

outputs2$COUNT <- as.numeric(outputs$COUNT)

outputs_wide <- outputs2 %>%
  group_by(GEOID, FLD_ZONE) %>%
  summarise(COUNT = sum(COUNT, na.rm = TRUE), .groups = "drop") %>%
  pivot_wider(
    names_from = FLD_ZONE,
    values_from = COUNT,
    values_fill = list(COUNT = 0)
  )

```

Here duplicative DFIRM values are consolidated for join. This is an artifact of independent cities within counties having distinct or recoded DFIRM6 values that do not fully align with their FIPS5.

```{r join_nfhlouttonoaaevent}

library(dplyr)

events_flood_df <- storm_details_df %>%
  left_join(outputs_wide, by = c("FIPS5" = "GEOID"))

subset_na_A <- events_flood_df %>%
  filter(is.na(A)) %>%
  dplyr::select(FIPS5, STATE, CZ_NAME)

distinct_na_A_FIPS5 <- events_flood_df %>%
  filter(is.na(A)) %>%
  dplyr::select(FIPS5, STATE, CZ_NAME) %>%
  distinct(FIPS5, .keep_all = TRUE)

```

Here we implement the join described above using the link table, and we extract unmatched IDs from each for exploration (which is how the manual lookup values were derived above)


```{r write_eventsflood, eval=FALSE}
write.csv(events_flood_df, file = paste0(data_dir,"/events_flood_df.csv"), row.names = FALSE)
```

Joined table is written. This block will not need to be run if the user has events_flood_df in their user data directory.

```{r load_eventsflood}
events_flood_df <- read_csv(paste0(data_dir,"/events_flood_df.csv"), col_types = cols(.default = "c"))

events_flood_df <- events_flood_df %>%
  mutate(
    YEAR = as.numeric(as.character(YEAR)),
    TOTAL_DAMAGE = as.numeric(as.character(TOTAL_DAMAGE)),
    DAMAGE_PROPERTY_CLEAN = as.numeric(as.character(DAMAGE_PROPERTY_CLEAN)),
    DAMAGE_CROPS_CLEAN = as.numeric(as.character(DAMAGE_CROPS_CLEAN)),
    DEATHS_DIRECT = as.numeric(as.character(DEATHS_DIRECT)),
    DEATHS_INDIRECT = as.numeric(as.character(DEATHS_INDIRECT)),
    INJURIES_DIRECT = as.numeric(as.character(INJURIES_DIRECT)),
    INJURIES_INDIRECT = as.numeric(as.character(INJURIES_INDIRECT))
  )

```

Joined table is loaded and fields retyped appropriately.


- Count flood event frequency per spatial unit.
- Overlay flood event hotspots on FEMA SFHA zones.
- Identify:
  - **Matching areas**: events occurring inside SFHA zones
  - **Mismatch areas**: events occurring outside SFHA zones

# 4. Time Series Analysis

- Aggregate flood events by decade.

```{r transform_eventsflood_decadeadd}
events_flood_df <- events_flood_df %>%
  mutate(
    DECADE = case_when(
      YEAR >= 1990 & YEAR < 2000 ~ "90s",
      YEAR >= 2000 & YEAR < 2010 ~ "00s",
      YEAR >= 2010 & YEAR < 2020 ~ "10s",
      YEAR >= 2020 & YEAR < 2030 ~ "2020-2024",
      TRUE ~ "Other"
    )
  ) %>%
  mutate(
    DECADE = factor(DECADE, levels = c("90s", "00s", "10s", "2020-2024"))
  )
```

Aggregate data by decade for 1990s to 2020s (to-date).

- Plot trends in:
  - Event frequency
  - Severity using damage amounts

```{r pivot_eventsflood_fips5decade}

decade_frequency_f <- events_flood_df %>%
  group_by(FIPS5, DECADE) %>%
  summarise(
    UNIQUE_EVENTS = n_distinct(EVENT_ID),
    .groups = "drop"
  )

# Summarize total damages by year and state
decade_damage_f <- events_flood_df %>%
  group_by(FIPS5,DECADE) %>%
  summarise(
    TOTAL_DAMAGE = sum(DAMAGE_PROPERTY_CLEAN + DAMAGE_CROPS_CLEAN, na.rm = TRUE),
    .groups = "drop"
  )

decade_casualty_f <- events_flood_df %>%
  group_by(FIPS5,DECADE) %>%
  summarise(
    TOTAL_CASUALTIES = sum(
      DEATHS_DIRECT + DEATHS_INDIRECT + INJURIES_DIRECT + INJURIES_INDIRECT,
      na.rm = TRUE),
    .groups = "drop"
  )

```

Here we create pivots for FIPS5 decade totals for damage (cost), event count, and casualties.

```{r pivot_eventsflood_totaldecade}
decade_frequency_t <- events_flood_df %>%
  group_by(DECADE) %>%
  summarise(
    UNIQUE_EVENTS = n_distinct(EVENT_ID),
    .groups = "drop"
  )

decade_damage_t <- events_flood_df %>%
  group_by(DECADE) %>%
  summarise(
    TOTAL_DAMAGE = sum(DAMAGE_PROPERTY_CLEAN + DAMAGE_CROPS_CLEAN, na.rm = TRUE),
    .groups = "drop"
  )

decade_casualty_t <- events_flood_df %>%
  group_by(DECADE) %>%
  summarise(
    TOTAL_CASUALTIES = sum(
      DEATHS_DIRECT + DEATHS_INDIRECT + INJURIES_DIRECT + INJURIES_INDIRECT,
      na.rm = TRUE),
    .groups = "drop"
  )
```

Here we create pivots for DMV decade totals for damage (cost), event count, and casualties.

```{r viz_eventsflood_decadedamage}
dmg_fips <- ggplot(decade_damage_f, aes(x = DECADE, y = log(TOTAL_DAMAGE+1))) +
  geom_boxplot(outlier.alpha = 0.3, fill = "#4682b4") +
  labs(
    title = "Distribution of Total Flood Event Damage\nby FIPS5 (County) by Decade",
    x = "Decade",
    y = "Log Total Flood Event Damage by FIPS5 (County)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

dmg_tot <- ggplot(decade_damage_t, aes(x = DECADE, y = log(TOTAL_DAMAGE + 1))) +
  geom_col(fill = "#4682b4") +
  labs(
    title = "Total Flood Damage by Decade",
    x = "Decade",
    y = "Log Total Flood Damage"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

library(gridExtra)
grid.arrange(dmg_fips, dmg_tot, ncol = 2, widths = c(1, 1))

zero_damage_count <- decade_damage_f %>%
  filter(TOTAL_DAMAGE == 0) %>%
  nrow()

fips_zerodamage <- zero_damage_count
fipscount <- length(decade_damage_f$FIPS5)

propzerodamage <- fips_zerodamage/fipscount

```

In the above FIPS5 and DMV Decade Total Flood Events time series aggregations we can see that individual counties saw a drop in estimated damage (in log dollars) due to flooding in the 2000s when compared to the 1990s, however the trend is generally positive, and 2020-2024 total damage cost is already almost equivalent to the 2010s, meaning that flooding events have become more severe over time in terms of damage expenses. A log transformation is applied as roughly `r round(propzerodamage,2)` of event observations have no estimated damage.

```{r viz_eventsflood_decadefrequency}

freq_fips <- ggplot(decade_frequency_f, aes(x = DECADE, y = UNIQUE_EVENTS)) +
  geom_boxplot(outlier.alpha = 0.3, fill = "#4682b4") +
  labs(
    title = "Distribution of Total Flood Events by\nFIPS5 (County) by Decade",
    x = "Decade",
    y = "Total Flood Events by FIPS5 (County)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

freq_tot <- ggplot(decade_frequency_t, aes(x = DECADE, y = UNIQUE_EVENTS)) +
  geom_col(fill = "#4682b4") +
  labs(
    title = "Distribution of Total Flood Events by\nFIPS5 (County) by Decade",
    x = "Decade",
    y = "Total Flood Events"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

grid.arrange(freq_fips, freq_tot, ncol = 2, widths = c(1, 1))
```
In the above FIPS5 and DMV Decade Total Flood Events time series aggregations we can see that individual counties are experiencing more frequent flooding each decade from the 90s to present, and that if the 2020s experience flooding for the remainder of the decade in line with what has already occured it will be the most flood-prone decade for the DMV of the 4 studied.

```{r viz_eventsflood_decadecasualty}

caus_fips <- ggplot(decade_casualty_f, aes(x = DECADE, y = TOTAL_CASUALTIES)) +
  geom_boxplot(outlier.alpha = 0.3, fill = "#4682b4") +
  labs(
    title = "Distribution of Total Flood Casualties by\nFIPS5 (County) by Decade",
    x = "Decade",
    y = "Total Flood Casualties by FIPS5 (County)"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

caus_tot <- ggplot(decade_casualty_t, aes(x = DECADE, y = TOTAL_CASUALTIES)) +
  geom_col(fill = "#4682b4") +
  labs(
    title = "Distribution of Total Flood Casualties by Decade",
    x = "Decade",
    y = "Total Flood Casualties"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

grid.arrange(caus_fips, caus_tot, ncol = 2, widths = c(1, 1))
```

In the above FIPS5 and DMV Decade Total casualty time series aggregations we can see that individual counties generally do not experience casualties during flood events, with a handful experiencing 0-5 total throughout the decade, and that so far the 2020s appear to be experiencing lower casualty totals during flood events when compared to previous decades.

######################################################################
######################################################################
######################################################################
###################### Andy Paladino's Section #######################
######################################################################
######################################################################
######################################################################

# Block Group Enrichment, Joins, and Aggregation 

```{r}

# all dependencies used

library(sf)
library(plotly)
sf_use_s2(FALSE)
library(dplyr)
library(stringr)
library(ggplot2)
library(gganimate)
library(sf)
library(readr)
library(tidyr)


dc <- read_csv("data/block_groups_dc.csv")
va <- read_csv("data/block_groups_va.csv")
md <- read_csv("data/block_groups_md.csv")
osm_buildings <- read_csv("data/building_footprints.csv")
noaa_flood_dc <- read_csv("data/noaa_flood_dc.csv")
storm_data <- read_delim("data/storm_details_dmv.csv", delim = "|")

bg_total <- bind_rows(dc, va, md)
bg_total_geo <- st_as_sf(bg_total, wkt = "geometry", crs = 4326)
bg_total_geo <- bg_total_geo %>%
  mutate(census_geometry = geometry)
bg_total_geo <- st_make_valid(bg_total_geo)

storm_data <- storm_data %>%
  filter(!is.na(BEGIN_LAT) & !is.na(BEGIN_LON))

storm_data_geo <- st_as_sf(storm_data, coords = c("BEGIN_LON", "BEGIN_LAT"), crs = 4326)
census_stormdata <- st_join(storm_data_geo, bg_total_geo, join = st_within)

census_stormdata <- census_stormdata %>%
  mutate(
    county_code = paste0(`attributes.STATE`, `attributes.COUNTY`),
    tract_code = paste0(`attributes.STATE`, `attributes.COUNTY`, `attributes.TRACT`)
  )

census_stormdata$BEGIN_DATE_TIME <- as.POSIXct(
  census_stormdata$BEGIN_DATE_TIME,
  format = "%d-%b-%y %H:%M:%S",
  tz = "UTC"
)

############################################################################
##### Mapping State Name and County Names to begin lat / begin long ########
############################################################################


# 1. Create the FIPS to State mapping
fips_to_state <- c(
  `11` = "DISTRICT OF COLUMBIA",
  `24` = "MARYLAND",
  `51` = "VIRGINIA"
)

# 2. Create Virginia, Maryland, and DC counties as dataframes
va_counties <- tibble(
  attributes.COUNTY = c(1, 3, 5, 7, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29,
                        31, 33, 35, 36, 37, 41, 43, 45, 47, 49, 51, 53, 57, 59, 61,
                        63, 65, 67, 69, 71, 73, 75, 77, 79, 81, 83, 85, 87, 89, 91,
                        93, 95, 97, 99, 101, 103, 105, 107, 109, 111, 113, 115, 117,
                        119, 121, 125, 127, 131, 133, 135, 137, 139, 141, 143, 145,
                        147, 149, 153, 155, 157, 159, 161, 163, 165, 167, 169, 171,
                        173, 175, 177, 179, 181, 183, 185, 187, 191, 193, 195, 197,
                        199, 510, 520, 530, 540, 550, 570, 590, 610, 620, 630, 640,
                        650, 660, 670, 680, 683, 690, 700, 710, 720, 730, 735, 740,
                        750, 760, 770, 775, 790, 800, 810, 820, 830, 840),
  County_Name = c(
    "Accomack", "Albemarle", "Alleghany", "Amelia", "Amherst",
    "Appomattox", "Arlington", "Augusta", "Bath", "Bedford",
    "Bland", "Botetourt", "Brunswick", "Buchanan", "Buckingham",
    "Campbell", "Caroline", "Carroll", "Charles City", "Charlotte",
    "Chesterfield", "Clarke", "Craig", "Culpeper", "Cumberland",
    "Dickenson", "Dinwiddie", "Essex", "Fairfax", "Fauquier",
    "Floyd", "Fluvanna", "Franklin", "Frederick", "Giles",
    "Gloucester", "Goochland", "Grayson", "Greene", "Greensville",
    "Halifax", "Hanover", "Henrico", "Henry", "Highland",
    "Isle of Wight", "James City", "King and Queen", "King George",
    "King William", "Lancaster", "Lee", "Loudoun", "Louisa",
    "Lunenburg", "Madison", "Mathews", "Mecklenburg", "Middlesex",
    "Montgomery", "Nelson", "New Kent", "Northampton", "Northumberland",
    "Nottoway", "Orange", "Page", "Patrick", "Pittsylvania",
    "Powhatan", "Prince Edward", "Prince George", "Prince William",
    "Pulaski", "Rappahannock", "Richmond", "Roanoke", "Rockbridge",
    "Rockingham", "Russell", "Scott", "Shenandoah", "Smyth",
    "Southampton", "Spotsylvania", "Stafford", "Surry", "Sussex",
    "Tazewell", "Warren", "Washington", "Westmoreland", "Wise",
    "Wythe", "York", "Alexandria City", "Bristol City", "Buena Vista City",
    "Charlottesville City", "Chesapeake City", "Colonial Heights City",
    "Danville City", "Falls Church City", "Franklin City", "Fredericksburg City",
    "Galax City", "Hampton City", "Harrisonburg City", "Hopewell City",
    "Lynchburg City", "Manassas City", "Martinsville City", "Newport News City",
    "Norfolk City", "Norton City", "Petersburg City", "Poquoson City",
    "Portsmouth City", "Radford City", "Richmond City", "Roanoke City",
    "Salem City", "Staunton City", "Suffolk City", "Virginia Beach City",
    "Waynesboro City", "Williamsburg City", "Winchester City"
  ),
  attributes.STATE = 51
)

md_counties <- tibble(
  attributes.COUNTY = c(1, 3, 5, 9, 11, 13, 15, 17, 19, 21, 23, 25, 27, 29, 31,
                        33, 35, 37, 39, 41, 43, 45, 47, 510),
  County_Name = c(
    "Allegany", "Anne Arundel", "Baltimore", "Calvert", "Caroline",
    "Carroll", "Cecil", "Charles", "Dorchester", "Frederick",
    "Garrett", "Harford", "Howard", "Kent", "Montgomery",
    "Prince Georges", "Queen Annes", "St. Marys", "Somerset",
    "Talbot", "Washington", "Wicomico", "Worcester", "Baltimore City"
  ),
  attributes.STATE = 24
)

dc_county <- tibble(
  attributes.COUNTY = 1,
  County_Name = "District of Columbia",
  attributes.STATE = 11
)

# 3. Combine all into one lookup
county_lookup <- bind_rows(va_counties, md_counties, dc_county)

# 4. Map State Name
census_stormdata <- census_stormdata %>%
  mutate(STATE_MAPPED = recode(as.character(attributes.STATE), !!!fips_to_state)) %>%
  mutate(attributes.COUNTY = as.numeric(attributes.COUNTY))

# 5. Merge county names
census_stormdata <- census_stormdata %>%
  left_join(county_lookup, by = c("attributes.STATE", "attributes.COUNTY"))

#########################################
###########. solely flooding events #####
#########################################

library(stringr)

# Define regex pattern
flood_keywords <- "flood|flash flood|flooding"

# Filter flood-related events
flood_events <- census_stormdata %>%
  filter(
    str_detect(EVENT_TYPE, regex(flood_keywords, ignore_case = TRUE)) |
    str_detect(EPISODE_NARRATIVE, regex(flood_keywords, ignore_case = TRUE)) |
    str_detect(EVENT_NARRATIVE, regex(flood_keywords, ignore_case = TRUE))
  )


#####################################
###########. aggregate event counts #
#####################################


# By YEAR, GIDBG, and census_geometry
bg_group <- census_stormdata %>%
  group_by(YEAR, GIDBG, census_geometry) %>%
  summarize(event_count = n(), .groups = "drop")

bg_group_flood <- flood_events %>%
  group_by(YEAR, GIDBG, census_geometry) %>%
  summarize(event_count = n(), .groups = "drop")

# Total events per GIDBG and census_geometry
sum_bg_group <- bg_group %>%
  group_by(GIDBG, census_geometry) %>%
  summarize(total_events = sum(event_count), .groups = "drop")

sum_bg_group_flood <- bg_group_flood %>%
  group_by(GIDBG, census_geometry) %>%
  summarize(total_events = sum(event_count), .groups = "drop")

# By YEAR and tract_code
tract_group <- census_stormdata %>%
  group_by(YEAR, tract_code) %>%
  summarize(event_count = n(), .groups = "drop")

tract_group_flood <- flood_events %>%
  group_by(YEAR, tract_code) %>%
  summarize(event_count = n(), .groups = "drop")

# By YEAR and county_code
county_group <- census_stormdata %>%
  group_by(YEAR, county_code) %>%
  summarize(event_count = n(), .groups = "drop")

county_group_flood <- flood_events %>%
  group_by(YEAR, county_code) %>%
  summarize(event_count = n(), .groups = "drop")

# By YEAR and STATE
state_group <- census_stormdata %>%
  group_by(YEAR, STATE) %>%
  summarize(event_count = n(), .groups = "drop")

state_group_flood <- flood_events %>%
  group_by(YEAR, STATE) %>%
  summarize(event_count = n(), .groups = "drop")
```

# County Level Enrichment, Joins, and Aggregation 

```{r}

library(dplyr)
library(readr)
library(sf)
library(stringr)


dc_counties <- read_csv('data/dc_counties.csv') %>%
  mutate(attributes.FUNCSTAT = as.character(attributes.FUNCSTAT))
va_counties <- read_csv('data/virginia_counties.csv') %>%
  mutate(attributes.FUNCSTAT = as.character(attributes.FUNCSTAT))
md_counties <- read_csv('data/md_counties.csv') %>%
  mutate(attributes.FUNCSTAT = as.character(attributes.FUNCSTAT))


county_total <- bind_rows(dc_counties, va_counties, md_counties) %>%
  mutate(geometry = st_as_sfc(geometry, crs = 4326))

county_total_geo <- st_as_sf(county_total, crs = 4326)
county_total_geo$census_geometry <- county_total_geo$geometry
county_stormdata <- st_join(storm_data_geo, county_total_geo, join = st_within)


flood_keywords <- c("flood", "flash flood", "flooding", "coastal flood")
pattern <- str_c(flood_keywords, collapse = "|")  

flood_events_county <- county_stormdata %>%
  filter(
    str_detect(EVENT_TYPE, regex(pattern, ignore_case = TRUE)) |
    str_detect(EPISODE_NARRATIVE, regex(pattern, ignore_case = TRUE)) |
    str_detect(EVENT_NARRATIVE, regex(pattern, ignore_case = TRUE))
  )

flood_events_county_group <- flood_events_county %>%
  group_by(attributes.NAME, census_geometry) %>%
  summarise(total_events = n(), .groups = "drop") %>%
  arrange(desc(total_events))

# 6. Save to CSV if needed
# write_csv(flood_events_county_group, "county_flood_events.csv")
```

# Event Counts Per County 

```{r}
library(ggplot2)
library(sf)
library(dplyr)

# Create the map
flood_map <- ggplot(flood_events_county_group) +
  geom_sf(aes(fill = total_events), color = "white", size = 0.2) +  # thinner county borders
  geom_sf_text(aes(label = attributes.NAME), size = 2.5, color = "white") +  # smaller font for county names
  scale_fill_viridis_c(option = "plasma", name = "Total Flood Events") +
  labs(
    title = "Total Flood Events by County",
    subtitle = "DMV Area",
    caption = "Source: Storm Data + Census Blockgroups"
  ) +
  theme_minimal(base_size = 10) +  # overall smaller base font
  theme(
    plot.title = element_text(face = "bold", size = 16, hjust = 0.5),
    plot.subtitle = element_text(size = 12, hjust = 0.5),
    legend.position = "right",
    panel.grid.major = element_line(color = "grey90", linetype = "dotted")
  ) +
  coord_sf(datum = NA)

# Display
print(flood_map)

# Optional: Save the plot MUCH bigger
# ggsave("/Users/andrewpaladino/Downloads/flood_events_county_map.png", plot = flood_map, width = 20, height = 15, dpi = 300)  # huge plot
```


# Line Plots / Timeseries Analysis of Event Group Per Block Groups Over Time 


```{r}

ggplot(bg_group, aes(x = YEAR, y = event_count, color = GIDBG, group = GIDBG)) +
  geom_line() +
  geom_point() +
  labs(title = "Event Count by GIDBG", x = "Year", y = "Event Count") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.3, "cm"),
    legend.text = element_text(size = 6)
  ) +
  guides(color = guide_legend(nrow = 3, byrow = TRUE))

ggplot(bg_group_flood, aes(x = YEAR, y = event_count, color = GIDBG, group = GIDBG)) +
  geom_line() +
  geom_point() +
  labs(title = "Event Count by GIDBG", x = "Year", y = "Event Count") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.3, "cm"),
    legend.text = element_text(size = 6)
  ) +
  guides(color = guide_legend(nrow = 3, byrow = TRUE))
```

# Line Plots / Timeseries Analysis of Event Group Per Tracts Over Time 

```{r}
ggplot(tract_group, aes(x = YEAR, y = event_count, color = tract_code, group = tract_code)) +
  geom_line() +
  geom_point() +
  labs(title = "Event Count by Tract", x = "Year", y = "Event Count") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.3, "cm"),
    legend.text = element_text(size = 6)
  ) +
  guides(color = guide_legend(nrow = 3, byrow = TRUE))

ggplot(tract_group_flood, aes(x = YEAR, y = event_count, color = tract_code, group = tract_code)) +
  geom_line() +
  geom_point() +
  labs(title = "Event Count by Tract", x = "Year", y = "Event Count") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.3, "cm"),
    legend.text = element_text(size = 6)
  ) +
  guides(color = guide_legend(nrow = 3, byrow = TRUE))
```

# Line Plots / Timeseries Analysis of Event Group Per County Over Time 

```{r}
ggplot(county_group, aes(x = YEAR, y = event_count, color = county_code, group = county_code)) +
  geom_line() +
  geom_point() +
  labs(title = "Event Count by Tract", x = "Year", y = "Event Count") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.3, "cm"),
    legend.text = element_text(size = 6)
  ) +
  guides(color = guide_legend(nrow = 3, byrow = TRUE))


ggplot(county_group_flood, aes(x = YEAR, y = event_count, color = county_code, group = county_code)) +
  geom_line() +
  geom_point() +
  labs(title = "Event Count by County", x = "Year", y = "Event Count") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.3, "cm"),
    legend.text = element_text(size = 6)
  ) +
  guides(color = guide_legend(nrow = 3, byrow = TRUE))
```

# Line Plots / Timeseries Analysis of Event Group Per State Over Time 

```{r}
ggplot(state_group, aes(x = YEAR, y = event_count, color = STATE, group = STATE)) +
  geom_line() +
  geom_point() +
  labs(title = "Event Count by Tract", x = "Year", y = "Event Count") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.3, "cm"),
    legend.text = element_text(size = 6)
  ) +
  guides(color = guide_legend(nrow = 3, byrow = TRUE))

ggplot(state_group_flood, aes(x = YEAR, y = event_count, color = STATE, group = STATE)) +
  geom_line() +
  geom_point() +
  labs(title = "Event Count by State", x = "Year", y = "Event Count") +
  theme_minimal() +
  theme(
    legend.position = "bottom",
    legend.key.size = unit(0.3, "cm"),
    legend.text = element_text(size = 6)
  ) +
  guides(color = guide_legend(nrow = 3, byrow = TRUE))

```

# Total Event Count vs. Flood Specific Event Count per County over Time

```{r}
ggplot(sum_bg_group) +
  geom_sf(aes(fill = total_events), color = NA, size = 0.4) +
  scale_fill_viridis_c(option = "plasma", name = "Event Count") +
  labs(
    title = "Total Events per Block Group",
    subtitle = "GIDBG-level Choropleth",
    caption = "Data Source: Your Project"
  ) +
  theme_minimal()


ggplot(sum_bg_group_flood) +
  geom_sf(aes(fill = total_events), color = NA, size = 0.4) +
  scale_fill_viridis_c(option = "plasma", name = "Event Count") +
  labs(
    title = "Total Events per Block Group",
    subtitle = "GIDBG-level Choropleth",
    caption = "Data Source: Your Project"
  ) +
  theme_minimal()
```

# Creating plot to include in the animation in the below cell, both are which we did not use in the final report

```{r}
#install.packages("gganimate")
#install.packages("transformr")  # needed for transitions
#install.packages("gifski")
#install.packages("png")
#install.packages("av")
#install.packages("ggspatial")
#install.packages("rosm")

library(ggplot2)
library(gganimate)
library(sf)


all_events_ani <- ggplot(bg_group) +
  geom_sf(data = bg_total_geo, fill = NA, color = "white", size = 0.2) +  # <- outline layer
  geom_sf(aes(fill = event_count), color = NA) +
  scale_fill_viridis_c(option = "plasma", name = "Event Count") +
  labs(
    title = 'Event Count by Block Group â€” Year: {current_frame}',
    subtitle = "Animated Choropleth",
    caption = "Data Source: Your Project"
  ) +
  theme_minimal() +
  transition_manual(frames = YEAR)



flood_events_ani <- ggplot(bg_group_flood) +
  geom_sf(data = bg_total_geo, fill = NA, color = "grey", size = 0.05) +  # <- outline layer
  geom_sf(aes(fill = event_count), color = NA) +
  scale_fill_viridis_c(option = "plasma", name = "Event Count") +
  labs(
    title = 'Flood Event Count by Block Group â€” Year: {current_frame}',
    subtitle = "Animated Choropleth",
    caption = "Data Source: Your Project"
  ) +
  theme_minimal() +
  transition_manual(frames = YEAR)

# Animation of Flood Event Counts per Block Group, which we did not include in the Report
#animate(all_events_ani, width = 1000, height = 800, fps = 2, renderer = gifski_renderer())
#animate(flood_events_ani, width = 1000, height = 800, fps = 2, renderer = gifski_renderer())

```


# Mapping FLOOD DAMAGE PROPERTY per block group ~ did not include in final report / analysis


```{r}
# 1. Filter non-missing DAMAGE_PROPERTY
flood_events_damage <- flood_events %>%
  filter(!is.na(DAMAGE_PROPERTY))

# 2. Convert DAMAGE_PROPERTY to numeric (handles K, M, B)
flood_events_damage <- flood_events_damage %>%
  mutate(
    DAMAGE_PROPERTY_NUM = case_when(
      str_detect(DAMAGE_PROPERTY, "K") ~ as.numeric(str_replace(DAMAGE_PROPERTY, "K", "")) * 1e3,
      str_detect(DAMAGE_PROPERTY, "M") ~ as.numeric(str_replace(DAMAGE_PROPERTY, "M", "")) * 1e6,
      str_detect(DAMAGE_PROPERTY, "B") ~ as.numeric(str_replace(DAMAGE_PROPERTY, "B", "")) * 1e9,
      TRUE ~ as.numeric(DAMAGE_PROPERTY)
    )
  )

# 3. Group and summarize, then remove zero-damage geometries
flood_events_damage_group <- flood_events_damage %>%
  group_by(GIDBG, census_geometry) %>%
  summarise(sumtotal_damage = sum(DAMAGE_PROPERTY_NUM, na.rm = TRUE), .groups = "drop") %>%
  filter(sumtotal_damage > 0)  # <- remove zeros

# 4. Sort descending by damage
flood_events_damage_group <- flood_events_damage_group %>%
  arrange(desc(sumtotal_damage))

ggplot(flood_events_damage_group) +
  geom_sf(aes(fill = sumtotal_damage), color = NA, size = 10) +
  scale_fill_viridis_c(option = "plasma", name = "Damage Totals") +
  labs(
    title = "Total Property Damage per Block Group",
    subtitle = "GIDBG-level Choropleth",
    caption = "Data Source: Your Project"
  ) +
  theme_minimal()

```

# Mapping Flood Damage Crops per block group over time, this analysis was also not included in final report / analysis

```{r}

# 1. Filter non-missing DAMAGE_CROPS
flood_events_crop <- flood_events %>%
  filter(!is.na(DAMAGE_CROPS))

# 2. Convert DAMAGE_CROPS to numeric
flood_events_crop <- flood_events_crop %>%
  mutate(
    DAMAGE_CROPS_NUM = case_when(
      str_detect(DAMAGE_CROPS, "K") ~ as.numeric(str_replace(DAMAGE_CROPS, "K", "")) * 1e3,
      str_detect(DAMAGE_CROPS, "M") ~ as.numeric(str_replace(DAMAGE_CROPS, "M", "")) * 1e6,
      str_detect(DAMAGE_CROPS, "B") ~ as.numeric(str_replace(DAMAGE_CROPS, "B", "")) * 1e9,
      TRUE ~ as.numeric(DAMAGE_CROPS)
    )
  )

# 3. Group by GIDBG and census_geometry, summarize total crop damage, remove zeros
flood_events_crop_group <- flood_events_crop %>%
  group_by(GIDBG, census_geometry) %>%
  summarise(sumtotal_crop_damage = sum(DAMAGE_CROPS_NUM, na.rm = TRUE), .groups = "drop") %>%
  filter(sumtotal_crop_damage > 0)

# 4. Sort descending
flood_events_crop_group <- flood_events_crop_group %>%
  arrange(desc(sumtotal_crop_damage))

ggplot(flood_events_crop_group) +
  geom_sf(aes(fill = sumtotal_crop_damage), color = NA, size = 0.1) +
  scale_fill_viridis_c(option = "plasma", name = "Crop Damage Totals") +
  labs(
    title = "Total Crop Damage per Block Group",
    subtitle = "GIDBG-level Choropleth",
    caption = "Data Source: Your Project"
  ) +
  theme_minimal()
```

# Mapping Magnitude of flood events per block group over time. 

```{r}
# 1. Filter non-missing MAGNITUDE
flood_events_mag <- flood_events %>%
  filter(!is.na(MAGNITUDE))

# 2. Group and summarize average magnitude
flood_events_mag_group <- flood_events_mag %>%
  group_by(GIDBG, census_geometry) %>%
  summarise(avg_magnitude = mean(MAGNITUDE, na.rm = TRUE), .groups = "drop") %>%
  filter(avg_magnitude > 0)

# 3. Sort descending
flood_events_mag_group <- flood_events_mag_group %>%
  arrange(desc(avg_magnitude))

ggplot(flood_events_mag_group) +
  geom_sf(aes(fill = avg_magnitude), color = NA, size = 0.1) +
  scale_fill_viridis_c(option = "plasma", name = "Avg. Magnitude") +
  labs(
    title = "Average Event Magnitude per Block Group",
    subtitle = "GIDBG-level Choropleth",
    caption = "Data Source: Your Project"
  ) +
  theme_minimal()
```

# Computing Summary Tables Total Events using aggregation functions

```{r}
# Grouping by full granularity: state, county, tract, block group
total_events_block <- census_stormdata %>%
  group_by(STATE_MAPPED, attributes.STATE, County_Name, attributes.COUNTY, attributes.TRACT, attributes.BLKGRP) %>%
  summarise(total_event_count_per_block = n(), .groups = "drop") %>%
  arrange(desc(total_event_count_per_block))

# Grouping by tract
total_events_tracks <- census_stormdata %>%
  group_by(STATE_MAPPED, attributes.STATE, County_Name, attributes.COUNTY, attributes.TRACT) %>%
  summarise(total_event_count_per_tract = n(), .groups = "drop") %>%
  arrange(desc(total_event_count_per_tract))

# Grouping by county
total_events_county <- census_stormdata %>%
  group_by(STATE_MAPPED, attributes.STATE, County_Name, attributes.COUNTY) %>%
  summarise(total_event_count_per_county = n(), .groups = "drop") %>%
  arrange(desc(total_event_count_per_county))

# Grouping by state only
total_events_state <- census_stormdata %>%
  group_by(STATE_MAPPED, attributes.STATE) %>%
  summarise(total_event_count_per_state = n(), .groups = "drop") %>%
  arrange(desc(total_event_count_per_state))

total_events_block
total_events_tracks
total_events_county
total_events_state
```

# Creating Summary Table of Flood Events per census level over time 

```{r}
library(dplyr)

# Block group level (STATE, COUNTY, TRACT, BLOCK GROUP)
flood_events_blocks <- flood_events %>%
  group_by(STATE_MAPPED, attributes.STATE, County_Name, attributes.COUNTY, attributes.TRACT, attributes.BLKGRP) %>%
  summarise(`Flood Event Count Per Block Group` = n(), .groups = "drop") %>%
  arrange(desc(`Flood Event Count Per Block Group`))

# Tract level (STATE, COUNTY, TRACT)
flood_events_tracts <- flood_events %>%
  group_by(STATE_MAPPED, attributes.STATE, County_Name, attributes.COUNTY, attributes.TRACT) %>%
  summarise(`Flood Event Count per Tract` = n(), .groups = "drop") %>%
  arrange(desc(`Flood Event Count per Tract`))

# County level (STATE, COUNTY)
flood_events_counties <- flood_events %>%
  group_by(STATE_MAPPED, attributes.STATE, County_Name, attributes.COUNTY) %>%
  summarise(`Flood Event Count per County` = n(), .groups = "drop") %>%
  arrange(desc(`Flood Event Count per County`))

# State level (STATE)
flood_events_states <- flood_events %>%
  group_by(STATE_MAPPED, attributes.STATE) %>%
  summarise(`Flood Event Count per State` = n(), .groups = "drop") %>%
  arrange(desc(`Flood Event Count per State`))

flood_events_blocks
flood_events_tracts
flood_events_counties
flood_events_states
```


# Piecing it all together !!!
### Here we wrote the logic in python to join hazard layers into our table of flood events + block groups. We then computed an aggregation so that on a per block group basis we had total storm event counts and total mapped areas (square meters of flood zones). This logic is included in the commented out section of code below. We imported the resulting table (final table summary) into this environment, where we created a vulnerability score into our table and then enriched our table with block group demographics to analyze socialeconomic trends.  


```{r}
library(sf)
library(dplyr)
library(readr)
library(tidyr)

# 
# # Python logic prior to import
# import pandas as pd
# import geopandas as geo
# from shapely.wkt import loads
# 
# # 1. Load flood hazard layer
# path = 'data/flood_insurance_hazard_all.csv'
# flood_hazard = pd.read_csv(path)
# 
# 
# flood_hazard = flood_hazard[flood_hazard['properties.SFHA_TF'] == 'T']
# flood_hazard['geometry'] = flood_hazard['geometry'].apply(lambda x: loads(x) if pd.notnull(x) else None)
# flood_hazard_geo = geo.GeoDataFrame(flood_hazard, geometry='geometry', crs='EPSG:4326')
# flood_hazard_geo = flood_hazard_geo.drop(columns=['index_left', 'index_right'], errors='ignore')
# flood_events = flood_events.drop(columns=['index_left', 'index_right'], errors='ignore')
# 
# flood_events_flood_hazard = geo.sjoin(flood_events, flood_hazard_geo, how='left', predicate='within')
# flood_summary = flood_events_flood_hazard.groupby(['GIDBG', 'census_geometry']).agg(
#     total_flood_events=('geometry', 'count'),                     
#     unique_flood_zones=('properties.DFIRM_ID', 'nunique'),         
#     unique_zone_types=('properties.FLD_ZONE', lambda x: list(x.dropna().unique())),
#     unique_flooding_risk_types=('properties.esri_symbology', lambda x: list(x.dropna().unique()))
# ).reset_index()
# 
# flood_summary_gdf = geo.GeoDataFrame(
#     flood_summary,
#     geometry=flood_summary['census_geometry'],
#     crs='EPSG:4326'
# ).drop(columns='census_geometry')
# 
# 
# flood_hazard_geo_proj = flood_hazard_geo.to_crs(epsg=6933)
# flood_summary_gdf_proj = flood_summary_gdf.to_crs(epsg=6933)
# flood_hazard_clipped = geo.overlay(flood_hazard_geo_proj, flood_summary_gdf_proj, how='intersection')
# flood_hazard_clipped['clipped_area'] = flood_hazard_clipped.geometry.area  # Now metersÂ²!
# flood_zone_area_summary = flood_hazard_clipped.groupby('GIDBG').agg(
#     total_flood_zone_area=('clipped_area', 'sum')
# ).reset_index()
# 
# final_summary = flood_summary_gdf.merge(flood_zone_area_summary, on='GIDBG', how='left')
# final_summary['total_flood_zone_area'] = final_summary['total_flood_zone_area'].fillna(0)
# 
# print(final_summary.head())
# final_summary.to_csv('final_table_summary.csv')



fh_path <- "data/flood_insurance_hazard_all.csv"
final_summary_path = 'data/final_table_summary.csv'
flood_hazard <- read_csv(fh_path)
final_summary = read_csv(final_summary_path)

library(readr)
library(dplyr)
library(ggplot2)
library(scales)
library(caret)   # for preProcess
library(ggpmisc) # for OLS trendline with equation


dc_attr <- read_csv("data/dc_attribution.csv")
md_attr <- read_csv("data/md_attribution.csv")
va_attr <- read_csv("data/va_attribution.csv")

total_attr <- bind_rows(dc_attr, md_attr, va_attr)

attributions_final_merge <- final_summary %>%
  left_join(total_attr, by = "GIDBG")

# Normalize total_flood_events and total_flood_zone_area
norm_vars <- attributions_final_merge %>%
  select(total_flood_events, total_flood_zone_area)

preproc <- preProcess(norm_vars, method = c("range"))  # Min-Max Scaling
norm_scores <- predict(preproc, norm_vars)

# Add normalized scores and vulnerability score
attributions_final_merge <- attributions_final_merge %>%
  mutate(
    event_score = norm_scores$total_flood_events,
    area_score = norm_scores$total_flood_zone_area,
    vulnerability_score = event_score * (1 - area_score)
  )

# Plot 1: Vulnerability vs Median Household Income
ggplot(attributions_final_merge, aes(x = vulnerability_score, y = Med_HHD_Inc_BG_ACS_17_21, color = Med_HHD_Inc_BG_ACS_17_21)) +
  geom_point(alpha = 0.7) +
  scale_color_viridis_c(option = "plasma") +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  labs(
    title = "Vulnerability Score vs Median Household Income",
    x = "Vulnerability Score",
    y = "Median Household Income"
  ) +
  theme_minimal()

# Plot 2: Vulnerability vs % Mobile Homes
ggplot(attributions_final_merge, aes(x = vulnerability_score, y = pct_Mobile_Homes_ACS_17_21, color = pct_Mobile_Homes_ACS_17_21)) +
  geom_point(alpha = 0.7) +
  scale_color_viridis_c(option = "plasma") +
  geom_smooth(method = "lm", se = FALSE, color = "black") +
  labs(
    title = "Vulnerability Score vs % Mobile Homes",
    x = "Vulnerability Score",
    y = "% Mobile Homes"
  ) +
  theme_minimal()
```

