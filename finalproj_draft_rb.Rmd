---
title: "6289_final_3"
author: "rballard"
date: "2025-04-15"
output: html_document
---


# PROJECT NAME

## Frontmatter
```{r ezidsetup,eval=FALSE}
#https://github.com/physicsland/ezids
if (!requireNamespace("remotes", quietly = TRUE)) { install.packages("remotes") }
install.packages ("devtools")
devtools::install_github("physicsland/ezids")
install.packages("ezids")
```
The above installs ezids, a package manager developed and maintained by GW DS Department.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

set.seed(20052)

#install.packages("rfema", repos = "https://ropensci.r-universe.dev")

library(ezids)
pkg_vector <- c("sf", "dplyr", "tmap", "terra", "NFHL", "exactextractr","spatialreg", "broom", "ggplot2","lmtest", "sf", "spData","spdep", "sphet", "INLA","NFHL","AOI", "tidycensus", "tigris","rfema","raster","stringr","xml2","rvest","httr","reader","purrr","readr")

invisible(lapply(pkg_vector,loadPkg))

data_dir <- "C:/Users/russe/Documents/github/6289spatial_final/data/"
```


# 1. FEMA Flood Zone Extraction

- Extract Layer 28 polygons for the DMV region using the `NFHL` R package.
- Filter for **Special Flood Hazard Areas (SFHA)** using the condition `SFHA_TF == "T"`.

# 2. NOAA Flood Event Collection

- Download and filter NOAA flood events (1950â€“2023) by event type:
  - Flood
  - Flash Flood
  - Coastal Flood

```{r noaa_bulkdatadownload, eval=FALSE}
#This code chunk will download all the NOAA event data so don't run if data is already present in environment.

# Base URL of the directory
base_url <- "https://www.ncei.noaa.gov/pub/data/swdi/stormevents/csvfiles/"

# Read the HTML content of the directory listing
page <- read_html(base_url)

# Extract all links to .csv.gz files
file_links <- page %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  na.omit() %>%
  str_subset("\\.csv\\.gz$")

# Make full URLs
full_urls <- paste0(base_url, file_links)

# Download folder (you can change this)
download_dir <- "C:/Users/russe/Documents/github/6289spatial_final/data/test"
dir.create(download_dir, showWarnings = FALSE)

# Loop to download with delays
for (url in full_urls) {
  file_name <- basename(url)
  dest_path <- file.path(download_dir, file_name)
  
  message("Downloading: ", file_name)
  tryCatch({
    download.file(url, destfile = dest_path, mode = "wb")
  }, error = function(e) {
    warning("Failed to download ", file_name, ": ", conditionMessage(e))
  })
  
  # Wait a few seconds to avoid hammering the server
  Sys.sleep(runif(1, 2, 5))  # random wait between 2 and 5 seconds
}

```

The above code chunk downloads NOAA Event Details for 1950-2024. It should only be run if data is not already present in development environment.

```{r noaa_rowjoin}
#rowwise joins of event details, fatalities, and locations


files <- list.files(data_dir, pattern = "\\.csv\\.gz$", full.names = TRUE)

details_files <- files[grepl("StormEvents_details", files)]
locations_files <- files[grepl("StormEvents_locations", files)]
fatalities_files <- files[grepl("StormEvents_fatalities", files)]

read_as_character <- function(file) {
  df <- read_csv(file, col_types = cols(.default = col_character()), show_col_types = FALSE)
  df$filename <- basename(file)
  df
}

# Combine all details files
storm_details_df <- map_dfr(details_files, read_as_character)

# Combine all locations files
storm_locations_df <- map_dfr(locations_files, read_as_character)

# Combine all fatalities files
storm_fatalities_df <- map_dfr(fatalities_files, read_as_character)

head(storm_details_df)

```

The above code chunk reads in and conducts a rowwise join of the annual NOAA event tables 

```{r noaa_tablewrite, eval=FALSE}
#writes processed table

write_pipe_csv_gz <- function(df, output_dir, filename) {
  # Ensure the output directory exists
  dir.create(output_dir, recursive = TRUE, showWarnings = FALSE)
  
  # Construct full path
  file_path <- file.path(output_dir, paste0(filename, ".csv.gz"))
  
  # Write the compressed, pipe-delimited file
  readr::write_delim(
    df,
    file = file_path,
    delim = "|",
    col_names = TRUE
  )
  
  message("File written to: ", file_path)
}

write_pipe_csv_gz(storm_details_dmv, data_dir, "storm_details_dmv")
```

The above chunk will write the joined noaa event details table to data_dir.


```{r noaa_tableload}
#will read the filtered storm_details_dmv.csv.gz saved to your repo data directory from GDrive

data_dir = "C:/Users/russe/Documents/github/6289spatial_final/data"

read_pipe_csv_gz <- function(input_dir, filename) {
  file_path <- file.path(input_dir, paste0(filename, ".csv.gz"))
  
  df <- readr::read_delim(
    file = file_path,
    delim = "|",
    col_names = TRUE,
    show_col_types = FALSE,
  col_types = cols(.default = col_character())
  )
  
  return(df)
}

storm_details_df <- as_tibble(read_pipe_csv_gz(data_dir, "storm_details_dmv"))
```

The above code chunk can be used to load a previously saved noaa event details .csv.gz to proceed with processing.

```{r noaa_detailsfilter}
#filters event details to include only DC, MD, VA state fips codes
storm_details_df <- storm_details_df %>%
  filter(STATE_FIPS %in% c("11", #DC
                           "24", #MD
                           "51"  #VA
                           ))

storm_details_df <- storm_details_df %>%
  filter(EVENT_TYPE %in% c('Flash Flood',
                           'Flood',
                           'Coastal Flood'
                           ))
```

The above code chunk filters the events data to include only STATE_FIPS for DC/MD/VA and to only include EVENT_TYPES of Coastal Flood, Flash Flood, or Flood.


- Extract:
  - Geolocation
  - Event date
  - Event type
  - Damage estimates

```{r noaatablesubset}
library(dplyr)
storm_details_df <- storm_details_df %>%
  dplyr::select("BEGIN_YEARMONTH",
                "BEGIN_DAY",
                "BEGIN_TIME",
                "STATE",
                "STATE_FIPS",
                "YEAR",
                "MONTH_NAME",
                "EVENT_TYPE",
                "CZ_FIPS",
                "CZ_NAME",
                "BEGIN_DATE_TIME",
                "INJURIES_DIRECT",
                "INJURIES_INDIRECT",
                "DEATHS_DIRECT",
                "DEATHS_INDIRECT",
                "DAMAGE_PROPERTY",
                "DAMAGE_CROPS",
                "FLOOD_CAUSE",
                "BEGIN_LOCATION",
                "BEGIN_LAT",
                "BEGIN_LON"
)


```


```{r noaaevents_cleaning}
storm_details_df <- storm_details_df %>%
  mutate(
    DAMAGE_CROPS_CLEAN = case_when(
      str_detect(DAMAGE_CROPS, regex("^[0-9.]+K$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_CROPS, regex("K", ignore_case = TRUE))) * 1e3,
      str_detect(DAMAGE_CROPS, regex("^[0-9.]+M$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_CROPS, regex("M", ignore_case = TRUE))) * 1e6,
      str_detect(DAMAGE_CROPS, "^[0-9.]+$") ~
        as.numeric(DAMAGE_CROPS),
      TRUE ~ NA_real_
    )
  )

storm_details_df <- storm_details_df %>%
  mutate(
    DAMAGE_PROPERTY_CLEAN = case_when(
      str_detect(DAMAGE_PROPERTY, regex("^[0-9.]+K$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_PROPERTY, regex("K", ignore_case = TRUE))) * 1e3,
      str_detect(DAMAGE_PROPERTY, regex("^[0-9.]+M$", ignore_case = TRUE)) ~
        as.numeric(str_remove(DAMAGE_PROPERTY, regex("M", ignore_case = TRUE))) * 1e6,
      str_detect(DAMAGE_PROPERTY, "^[0-9.]+$") ~
        as.numeric(DAMAGE_PROPERTY),
      TRUE ~ NA_real_
    )
  )


storm_details_df <- storm_details_df %>%
  mutate(across(c(INJURIES_DIRECT, INJURIES_INDIRECT, DEATHS_DIRECT, DEATHS_INDIRECT), as.numeric))


storm_details_df <- storm_details_df %>%
  mutate(
    FIPS5 = paste0(STATE_FIPS, str_pad(CZ_FIPS, width = 3, pad = "0"))
  )

storm_details_df <- storm_details_df %>%
  mutate(
    TOTAL_DAMAGE = DAMAGE_PROPERTY_CLEAN + DAMAGE_CROPS_CLEAN
  )
```


# 3. Spatial Join and Aggregation

- Assign events to spatial units:
  - Census tracts or
  - ZIP Code Tabulation Areas (ZCTAs)
- Count flood event frequency per spatial unit.
- Overlay flood event hotspots on FEMA SFHA zones.
- Identify:
  - **Matching areas**: events occurring inside SFHA zones
  - **Mismatch areas**: events occurring outside SFHA zones

# 4. Time Series Analysis

- Aggregate flood events by decade.
- Plot trends in:
  - Event frequency
  - Severity using damage amounts


```{r noaaevents_annualdamage}
# Summarize total damages by year and state
annual_damage <- storm_details_df %>%
  #filter(STATE_FIPS %in% c(11, 24, 51)) %>%
  #group_by(STATE_FIPS, YEAR) %>%
  group_by(YEAR) %>%
  summarise(
    TOTAL_DAMAGE = sum(DAMAGE_PROPERTY_CLEAN + DAMAGE_CROPS_CLEAN, na.rm = TRUE),
    .groups = "drop"
  )


# Plot as column chart
#ggplot(annual_damage, aes(x = YEAR, y = TOTAL_DAMAGE, fill = as.factor(STATE_FIPS))) +
ggplot(annual_damage, aes(x = YEAR, y = TOTAL_DAMAGE)) +
  geom_col(position = "dodge") +
  labs(
    title = "Annual Flood-Related Damage by State (DC, MD, VA)",
    x = "Year",
    y = "Total Damage (USD)",
    fill = "STATE_FIPS"
  ) +
  theme_minimal() +
  scale_y_continuous(labels = scales::dollar)

#ggplot(annual_damage, aes(x = YEAR, y = log10(TOTAL_DAMAGE+1), fill = as.factor(STATE_FIPS))) +
ggplot(annual_damage, aes(x = YEAR, y = log10(TOTAL_DAMAGE+1))) +
  geom_col(position = "dodge") +
  labs(
    title = "Annual Flood-Related Damage by State (Log Scale)",
    x = "Year",
    y = "Log10(Total Damage in USD)",
    fill = "STATE_FIPS"
  ) +
  theme_minimal()

```

```{r noaa_annualdamageboxplots}
# Step 2: Aggregate by FIPS5 and YEAR
annual_fips_damage <- storm_details_df %>%
  group_by(FIPS5, YEAR) %>%
  summarise(TOTAL_DAMAGE = sum(TOTAL_DAMAGE, na.rm = TRUE), .groups = "drop")

# Step 3: Plot boxplots
ggplot(annual_fips_damage, aes(x = factor(YEAR), y = TOTAL_DAMAGE)) +
  geom_boxplot(outlier.shape = 21, fill = "lightblue") +
  scale_y_log10(labels = scales::dollar) +  # log scale for readability
  labs(
    title = "Annual Distribution of Flood Damage by County (FIPS5)",
    x = "Year",
    y = "Total Damage (log scale)"
  ) +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


```{r noaaevents_annualcasualty}

library(dplyr)
library(ggplot2)

# Summarize total casualties by year and state
annual_total_casualties <- storm_details_df %>%
  #group_by(STATE_FIPS, YEAR) %>%
  group_by( YEAR) %>%
  summarise(
    TOTAL_CASUALTIES = sum(
      DEATHS_DIRECT + DEATHS_INDIRECT + INJURIES_DIRECT + INJURIES_INDIRECT,
      na.rm = TRUE
    ),
    .groups = "drop"
  )

# Plot as a column chart
ggplot(annual_total_casualties, aes(x = YEAR, y = TOTAL_CASUALTIES, fill = as.factor(STATE_FIPS))) +
  geom_col(position = "dodge") +
  labs(
    title = "Annual Flood-Related Casualties (Deaths + Injuries) by State",
    x = "Year",
    y = "Total Casualties",
    fill = "STATE_FIPS"
  ) +
  theme_minimal()


```






### Annual Total Events By Event By Year By FIPS
```{r annualevents}
library(dplyr)
library(tidyr)

# 1. Extract YEAR and filter for DMV
storm_summary <- storm_details_df %>%
  filter(STATE_FIPS %in% c("11", "24", "51")) %>%
  mutate(
    YEAR = substr(BEGIN_YEARMONTH, 1, 4)
  ) %>%
  group_by(STATE, YEAR, EVENT_TYPE) %>%
  summarise(total_events = n(), .groups = "drop")

# 2. Pivot wider: one row per (STATE, YEAR), columns for each EVENT_TYPE
storm_pivot <- storm_summary %>%
  pivot_wider(
    names_from = EVENT_TYPE,
    values_from = total_events,
    values_fill = 0
  )

```


```{r eventtotalviz}
library(tidyr)
library(dplyr)

storm_long <- storm_pivot %>%
  pivot_longer(
    cols = -c(STATE, YEAR),
    names_to = "EVENT_TYPE",
    values_to = "TOTAL_EVENTS"
  ) %>%
  mutate(YEAR = as.integer(YEAR))

library(ggplot2)

ggplot(storm_long, aes(x = YEAR, y = TOTAL_EVENTS, color = EVENT_TYPE)) +
  geom_line(size = 1) +
  facet_wrap(~ STATE) +
  labs(
    title = "Storm Events Over Time by Type",
    x = "Year",
    y = "Number of Events",
    color = "Event Type"
  ) +
  theme_minimal() +
  theme(
    text = element_text(size = 12),
    legend.position = "bottom"
  )


```


```{r statecountypivot}

library(dplyr)

cz_unique_count <- storm_details_df %>%
  group_by(STATE_FIPS) %>%
  summarise(unique_cz_fips = n_distinct(CZ_FIPS), .groups = "drop")

cz_unique_count
```


```{r cz_fipseventboxplotbyyear}
library(dplyr)
library(ggplot2)

# Step 1: Extract year and count events per CZ_FIPS per year
cz_events_per_year <- storm_details_df %>%
  mutate(YEAR = substr(BEGIN_YEARMONTH, 1, 4)) %>%
  group_by(YEAR, CZ_FIPS, STATE_FIPS) %>%
  summarise(total_events = n(), .groups = "drop")

ggplot(cz_events_per_year, aes(x = YEAR, y = total_events)) +
  geom_boxplot(outlier.alpha = 0.3, fill = "#69b3a2") +
  labs(
    title = "Distribution of Storm Events per CZ_FIPS by Year",
    x = "Year",
    y = "Total Events per CZ_FIPS"
  ) +
  theme_minimal() +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )
```

```{r cz_fipseventboxplotbydecade}
library(dplyr)

cz_events_per_decade <- storm_details_df %>%
  mutate(
    YEAR = as.integer(substr(BEGIN_YEARMONTH, 1, 4)),
    DECADE = paste0(floor(YEAR / 10) * 10, "s")
  ) %>%
  group_by(DECADE, CZ_FIPS, STATE_FIPS) %>%
  summarise(total_events = n(), .groups = "drop")


theme_landscape <- theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(size = 18, face = "bold"),
    axis.text.x = element_text(angle = 0),
    legend.position = "bottom"
  )

ggplot(cz_events_per_decade, aes(x = DECADE, y = total_events)) +
  geom_boxplot(outlier.alpha = 0.3, fill = "#4682b4") +
  labs(
    title = "Distribution of Storm Events per CZ_FIPS by Decade",
    x = "Decade",
    y = "Total Events per CZ_FIPS"
  ) +
  #theme_minimal() +
  theme_landscape +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 1),
    text = element_text(size = 12)
  )

```

```{r noaa_tablejoin,eval=FALSE}
#Joining these tables introduces cartesian joins into the data this should be approached with caution and intentionality.

# Start by joining details and locations (left join to keep all details)
details_locations <- left_join(
  storm_details_df,
  storm_locations_df,
  by = "EVENT_ID",
  suffix = c("_details", "_locations")
)

is_unique <- n_distinct(details_locations$EVENT_ID) == nrow(details_locations)

# Then join with fatalities
full_storm_df <- left_join(
  details_locations,
  storm_fatalities_df,
  by = "EVENT_ID",
  suffix = c("", "_fatalities")
)

# Join with nested fatalities
details_with_nested_fatalities <- left_join(details_locations, fatalities_nested, by = "EVENT_ID")

# Join fatalities to locations
fatalities_locations <- left_join(
  storm_fatalities_df,
  storm_locations_df,
  by = "EVENT_ID",
  suffix = c("_fatalities", "_locations")
)

full_storm_df <- left_join(
  storm_details_df,
  fatalities_locations,
  by = "EVENT_ID",
  suffix = c("_details", "_locations_fatalities")
)
```



---old---

```{r loaddata,eval=FALSE}

nfhl_describe(28)

dc_aoi <- AOI::aoi_get("Washington, DC") |> 
  AOI::aoi_buffer(dist = 10, unit = "km")

dc_aoi <- AOI::aoi_get("Washington, DC", wh = 10, units = "km", bbox = TRUE) |>
    st_as_sf()

md_aoi <- AOI::aoi_ext("MD", wh = 10, units = "km", bbox = TRUE) |>
    st_as_sf()

va_aoi <- AOI::aoi_ext("VA", wh = 10, units = "km", bbox = TRUE) |>
    st_as_sf()

mdva_aoi<- AOI::aoi_ext(state = c("VA", "MD"))
mdva_floodzone <- nfhl_get(dc_aoi, 28) %>%
                  filter(SFHA_TF == "T")


dc_floodzone <- nfhl_get(dc_aoi, 28) %>%
    filter(SFHA_TF == "T")
md_floodzone <- nfhl_get(md_aoi, 28)
va_floodzone <- nfhl_get(va_aoi, 28)



```

```{r samplecode, eval=FALSE}

#sample code
AOI <- AOI::aoi_ext("UCSB", wh = 10, units = "km", bbox = TRUE) |>
    st_as_sf()

# View at Layer ID 28 description
nfhl_describe(28)$Description
#> [1] "The S_Fld_Haz_Ar table contains information about the flood hazards within the flood risk project area. These zones are used by FEMA to designate the SFHA and for insurance rating purposes.  These data are the regulatory flood zones designated by FEMA."

# Extract Flood Hazard Polygons and filter by Special Flood
# Hazard Areas (SFHA)
floodhazard <- nfhl_get(AOI, 28) %>%
    filter(SFHA_TF == "T")


```

```{r testncload, eval=FALSE}
library("raster")
r = raster("C:\\Users\\russe\\Downloads\\gpcp_v01r03_daily_d20101231_c20170530.nc")

```

```{r testnoaaload,eval=FALSE}
df <- read.table("C:\\Users\\russe\\Downloads\\DC_Features_20140204.txt", 
                 header = TRUE, 
                 sep = "|",
                 quote="",
                 stringsAsFactors = FALSE)
```



#test flood layer extract

```{r nfhlextract_test}

library(sf)
library(dplyr)
library(tigris)    # for county boundaries
options(tigris_use_cache = TRUE)
library(httr)

# Load county geometries for DC, MD, VA
counties <- counties(state = c("11", "24", "51"), cb = TRUE, class = "sf")

getwd()

counties_df <- st_drop_geometry(counties)
write_csv(counties_df, "data/test/counties_dc_md_va.csv")

# Add WKT geometry as a new column
counties_wkt <- counties %>%
  mutate(geometry_wkt = st_as_text(geometry)) %>%
  st_drop_geometry()

# Write to CSV
write_csv(counties_wkt, "data/test/counties_dc_md_va_wkt.csv")





get_nfhl_by_county <- function(county_sf) {
  if (nrow(county_sf) != 1) stop("Please pass a single county geometry.")

  # Bounding box
  bbox <- st_bbox(county_sf)

  # FEMA NFHL Layer 28 API endpoint
  url <- "https://hazards.fema.gov/gis/nfhl/rest/services/public/NFHL/MapServer/28/query"

  # Create POST body (xMin, yMin, xMax, yMax)
  body <- list(
    f = "geojson",
    where = "1=1",
    geometryType = "esriGeometryEnvelope",
    geometry = paste0('{"xmin":', bbox["xmin"],
                      ',"ymin":', bbox["ymin"],
                      ',"xmax":', bbox["xmax"],
                      ',"ymax":', bbox["ymax"],
                      ',"spatialReference":{"wkid":4326}}'),
    inSR = "4326",
    spatialRel = "esriSpatialRelIntersects",
    outFields = "*",
    returnGeometry = "true"
  )

  # Send POST request
  res <- POST(url, body = body, encode = "form")

  if (status_code(res) == 200) {
    temp_file <- tempfile(fileext = ".geojson")
    writeBin(content(res, "raw"), temp_file)

    tryCatch({
      st_read(temp_file, quiet = TRUE)
    }, error = function(e) {
      message("Failed to parse GeoJSON for county: ", county_sf$NAME)
      NULL
    })
  } else {
    message("Request failed with status: ", status_code(res), " for county: ", county_sf$NAME)
    NULL
  }
}

# Test with one county (e.g., Arlington)
arlington <- counties[counties$NAME == "Arlington", ]
arlington_zones <- get_nfhl_by_county(arlington)


```




```{r createnfhldf}
library(rvest)
library(dplyr)
library(stringr)
library(tidyr)

# FEMA NFHL base URL
base_url <- "https://hazards.fema.gov/femaportal/NFHL/"
target_url <- paste0(base_url, "searchResult")

# Read page
page <- read_html(target_url)

# Extract all table rows
rows <- page %>% html_nodes("tr")

# Parse each row into fields: FIRMID, County, State, Date, Size, Link
nfhl_df <- rows %>%
  map_df(~{
    tds <- .x %>% html_nodes("td")
    if (length(tds) < 6) return(NULL)  # skip malformed rows

    list(
      dfirm_id = tds[1] %>% html_text(trim = TRUE),
      county = tds[2] %>% html_text(trim = TRUE),
      state = tds[3] %>% html_text(trim = TRUE),
      effective_date = tds[4] %>% html_text(trim = TRUE),
      file_size = tds[5] %>% html_text(trim = TRUE),
      url = tds[6] %>% html_node("a") %>% html_attr("href") %>% paste0(base_url, .)
    )
  }) %>%
  filter(str_detect(dfirm_id, "^(11|24|51).*")) %>%
  mutate(url = URLencode(url, reserved = TRUE))

# Preview
print(head(nfhl_df))




```


```{r nfhl_scrape}

library(rvest)
library(stringr)
library(purrr)

# Base URL and scrape target
base_url <- "https://hazards.fema.gov/femaportal/NFHL/"
target_url <- paste0(base_url, "searchResult")

# Read and parse HTML
page <- read_html(target_url)

# Extract all download links with their file names
zip_links <- page %>%
  html_nodes("a") %>%
  html_attr("href") %>%
  na.omit() %>%
  str_subset("^Download/ProductsDownLoadServlet\\?DFIRMID=") %>%
  unique()

# Filter by DFIRMID prefix: 11, 24, or 51
filtered_links <- zip_links %>%
  str_subset("^Download/ProductsDownLoadServlet\\?DFIRMID=(11|24|51)")

# Convert to full URLs
full_urls <- paste0(base_url, filtered_links)
full_urls <- gsub(" ", "%20", full_urls)


# Optional: Set destination folder
dest_dir <- "data/test/nfhl_zips"
dir.create(dest_dir, showWarnings = FALSE)

# Download function with polite wait
download_nfhl_zip <- function(url, delay = 3) {
  file_name <- str_extract(url, "fileName=[^&]+") %>%
    str_remove("fileName=")
  dest_file <- file.path(dest_dir, file_name)

  if (!file.exists(dest_file)) {
    message("Downloading: ", file_name)
    tryCatch({
      download.file(url, destfile = dest_file, mode = "wb")
      Sys.sleep(delay)
    }, error = function(e) {
      message("Error downloading ", file_name)
    })
  } else {
    message("Already downloaded: ", file_name)
  }
}

# Loop and download
walk(full_urls, download_nfhl_zip)

```


```{r read shapefile}
library(sf)
library(dplyr)

# Example: pick one ZIP file
zip_path <- "data/test/nfhl_zips/24001C_20231112.zip"  # Update path to your example

# Temporary folder to extract to
extract_dir <- tempfile(pattern = "nfhl_")
dir.create(extract_dir)

# Unzip contents
unzip(zip_path, exdir = extract_dir)

# List extracted files
extracted_files <- list.files(extract_dir, recursive = TRUE, full.names = TRUE)

# Find the .shp file (or pick manually if multiple)
shp_file <- extracted_files[grepl("\\.shp$", extracted_files, ignore.case = TRUE)]

# Just for clarity: if multiple SHPs exist, pick one:
# (Here just picking the first)
shp_file <- shp_file[3]

# Read shapefile with sf
flood_layer <- st_read(shp_file)

# Preview
print(flood_layer)



testdf <- flood_layer$FLD_ZONE


simple_df <- flood_layer %>%
  dplyr::select(FLD_ZONE, FLD_AR_ID) %>%
  st_drop_geometry()

simple_df %>%
  count(FLD_ZONE, name = "count") %>%
  arrange(desc(count))


# flood_layer %>%
#   count(FLD_ZONE, name = "count") %>%
#   arrange(desc(count))
```

